Project Code Export - Category: Backend_App
Generated for Gemini Canvas Analysis


==================================================
FILE_PATH: app\clients\genai.py
==================================================

import os
import re
from dotenv import load_dotenv
from google import genai
from google.genai.types import Tool, GenerateContentConfig, GoogleSearch
from app.util.decorators import perform_async_logging

load_dotenv()

client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

@perform_async_logging
async def genai_generate_text(prompt: str):
    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=[{"role": "user", "parts": [{"text": prompt}]}],
        config={
            "response_mime_type": "text/plain", # JSON ê°•ì œ ì œê±° (ìœ ì—°ì„± í™•ë³´)
        }
    )
    # response.textê°€ Noneì¼ ê²½ìš° ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    result_text = response.text if response.text else ""
    return result_text.strip()

@perform_async_logging
async def genai_generate_with_grounding(prompt: str):
    """
    Google Search Groundingì„ ì‚¬ìš©í•˜ì—¬ ìµœì‹  ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³  ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    (Dynamic Retrieval ì„¤ì •)
    """
    # Google Search ë„êµ¬ ì„¤ì •
    google_search_tool = Tool(
        google_search=GoogleSearch()
    )

    response = client.models.generate_content(
        model="gemini-2.0-flash",
        contents=[{"role": "user", "parts": [{"text": prompt}]}],
        config=GenerateContentConfig(
            tools=[google_search_tool],
            response_modalities=["TEXT"],
        )
    )
    
    # Grounding ë©”íƒ€ë°ì´í„° (ì†ŒìŠ¤ ì¶œì²˜ ë“±) ì¶”ì¶œ
    citations = []
    if response.candidates and response.candidates[0].grounding_metadata:
        metadata = response.candidates[0].grounding_metadata
        if metadata.grounding_chunks:
            for chunk in metadata.grounding_chunks:
                if chunk.web:
                    title = chunk.web.title or "Link"
                    uri = chunk.web.uri
                    citations.append(f"- [{title}]({uri})")
    
    result_text = response.text if response.text else "ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
    
    # ì¶œì²˜ ëª©ë¡ì´ ìˆìœ¼ë©´ í•˜ë‹¨ì— ì¶”ê°€
    if citations:
        # ì¤‘ë³µ ì œê±°
        unique_citations = list(dict.fromkeys(citations))
        result_text += "\n\n**ğŸŒ ì°¸ê³  ì¶œì²˜:**\n" + "\n".join(unique_citations)
        
    return result_text.strip()



==================================================
FILE_PATH: app\clients\openai.py
==================================================

import os
from dotenv import load_dotenv
from openai import AsyncOpenAI
from app.util.decorators import perform_async_logging

load_dotenv()

client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

@perform_async_logging
async def openai_generate_text(prompt: str, model: str = "gpt-4o"):
    response = await client.chat.completions.create(
        model=model,
        messages=[
            {"role": "user", "content": prompt}
        ],
        temperature=0.7
    )
    return response.choices[0].message.content


==================================================
FILE_PATH: app\clients\tavily.py
==================================================

import os
from dotenv import load_dotenv
from app.util.decorators import perform_async_logging

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# Tavily API í‚¤ (í•˜ë“œì½”ë”© ëœ í‚¤ê°€ ìˆë‹¤ë©´ ìš°ì„ ìˆœìœ„ ì£¼ì˜, ì—¬ê¸°ì„  .env ì‚¬ìš© ê¶Œì¥)
# .envì— TAVILY_API_KEYê°€ ì—†ìœ¼ë©´ ì•„ë˜ í•˜ë“œì½”ë”© ëœ í‚¤ë¥¼ fallbackìœ¼ë¡œ ì‚¬ìš©
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY", "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")

@perform_async_logging
async def tavily_search(query: str, max_results: int = 5):
    """
    Tavily APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´(Context ë¶€ì¡± ì‹œ) ì¿¼ë¦¬ë¥¼ ë‹¨ìˆœí™”í•˜ì—¬ ì¬ì‹œë„(Self-Correction)í•©ë‹ˆë‹¤.
    
    Returns:
        str: í¬ë§·íŒ…ëœ ê²€ìƒ‰ ê²°ê³¼ ë¬¸ìì—´ (Title, Link, Snippet í¬í•¨)
    """
    try:
        from tavily import TavilyClient
        tavily = TavilyClient(api_key=TAVILY_API_KEY)
        
        # 1ì°¨ ê²€ìƒ‰ ì‹œë„ (êµ¬ì²´ì  ì¿¼ë¦¬)
        # ì˜ˆ: "ì¹´í˜ í”„ëœì°¨ì´ì¦ˆ [ì§ˆë¬¸]" í˜•íƒœë¡œ ë¬¸ë§¥ ë³´ê°•
        target_query = f"ì¹´í˜ í”„ëœì°¨ì´ì¦ˆ {query}"
        response = tavily.search(query=target_query, search_depth="basic", max_results=max_results)
        raw_results = response.get('results', [])
        
        # ğŸ”„ Self-Correction: ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¿¼ë¦¬ ë‹¨ìˆœí™” í›„ ì¬ì‹œë„
        if not raw_results:
            print(f"ğŸ”„ [Tavily Correction] '{target_query}' ê²°ê³¼ ì—†ìŒ -> '{query}' ì¬ê²€ìƒ‰")
            # ì ‘ë‘ì‚¬ ì œê±°í•˜ê³  ì§ˆë¬¸ ìì²´ë¡œ ê²€ìƒ‰
            response = tavily.search(query=query, search_depth="basic", max_results=max_results)
            raw_results = response.get('results', [])
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_list = []
        for item in raw_results:
            title = item.get('title', 'ì œëª© ì—†ìŒ')
            url = item.get('url', '#')
            content = item.get('content', '')
            formatted_list.append(f"Title: {title}\nLink: {url}\nSnippet: {content}\n")
        
        if not formatted_list:
             return "Tavily ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        else:
             return "[ì™¸ë¶€ ì›¹ ê²€ìƒ‰ ê²°ê³¼ (Tavily)]\n" + "\n---\n".join(formatted_list)
             
    except Exception as e:
        print(f"âŒ Tavily ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return f"Tavily ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"



==================================================
FILE_PATH: app\clients\weather.py
==================================================

import httpx
from datetime import date, datetime

# WMO ê¸°ìƒ ì½”ë“œ ë§¤í•‘ (Open-Meteo ê¸°ì¤€)
def map_wmo_code(code: int) -> str:
    if code == 0:
        return "ë§‘ìŒ"
    elif code in [1, 2, 3]:
        return "êµ¬ë¦„"
    elif code in [45, 48]:
        return "ì•ˆê°œ"
    elif code in [51, 53, 55, 61, 63, 65, 80, 81, 82]:
        return "ë¹„"
    elif code in [71, 73, 75, 77, 85, 86]:
        return "ëˆˆ"
    elif code in [95, 96, 99]:
        return "ë‡Œìš°/ë¹„"
    else:
        return "íë¦¼"

async def fetch_weather_data(dates: list, lat: float = 37.5665, lon: float = 126.9780) -> dict:
    """
    Open-Meteo Archive APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ê³¼ê±° ë‚ ì”¨ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.
    API Keyê°€ í•„ìš” ì—†ìœ¼ë©° ë¬´ë£Œì…ë‹ˆë‹¤.
    
    Args:
        dates (list): datetime.date ê°ì²´ ë˜ëŠ” 'YYYY-MM-DD' ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        lat (float): ìœ„ë„ (ê¸°ë³¸ê°’: ì„œìš¸ ì‹œì²­)
        lon (float): ê²½ë„ (ê¸°ë³¸ê°’: ì„œìš¸ ì‹œì²­)
        
    Returns:
        dict: { "2024-01-01": "ë§‘ìŒ", ... }
    """
    if not dates:
        return {}

    # 1. ë‚ ì§œ í¬ë§· í†µì¼ ë° ë²”ìœ„ ì„¤ì •
    # ì…ë ¥ëœ ë‚ ì§œ ì¤‘ ê°€ì¥ ê³¼ê±°ì™€ ê°€ì¥ ë¯¸ë˜ë¥¼ ì°¾ì•„ API ìš”ì²­ ë²”ìœ„ë¥¼ ì •í•©ë‹ˆë‹¤.
    sorted_dates = sorted([str(d) for d in dates])
    start_date = sorted_dates[0]
    end_date = sorted_dates[-1]

    # 2. Open-Meteo API í˜¸ì¶œ (ê³¼ê±° ë°ì´í„° ì•„ì¹´ì´ë¸Œ)
    # daily=weather_code : í•˜ë£¨ ëŒ€í‘œ ê¸°ìƒ ì½”ë“œë§Œ ìš”ì²­
    url = "https://archive-api.open-meteo.com/v1/archive"
    params = {
        "latitude": lat,
        "longitude": lon,
        "start_date": start_date,
        "end_date": end_date,
        "daily": "weather_code",
        "timezone": "auto"
    }

    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(url, params=params, timeout=5.0)
            response.raise_for_status()
            data = response.json()

        # 3. ê²°ê³¼ íŒŒì‹± ë° ë§¤í•‘
        daily_data = data.get("daily", {})
        time_list = daily_data.get("time", []) # ["2024-01-01", "2024-01-02", ...]
        code_list = daily_data.get("weather_code", []) # [0, 3, 61, ...]

        result_map = {}
        for date_str, code in zip(time_list, code_list):
            if code is not None:
                result_map[date_str] = map_wmo_code(code)
            else:
                result_map[date_str] = "ì•Œìˆ˜ì—†ìŒ"

        # ìš”ì²­í•œ ë‚ ì§œë“¤ì— ëŒ€í•´ì„œë§Œ í•„í„°ë§í•˜ì—¬ ë°˜í™˜ (APIëŠ” ë²”ìœ„ ì „ì²´ë¥¼ ì£¼ë¯€ë¡œ)
        final_result = {d: result_map.get(d, "ê¸°ë¡ì—†ìŒ") for d in sorted_dates}
        return final_result

    except Exception as e:
        print(f"âŒ [Weather] API í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")
        # ì‹¤íŒ¨ ì‹œ ë¹ˆ ê°’ ëŒ€ì‹  'ì•Œìˆ˜ì—†ìŒ' ë°˜í™˜í•˜ì—¬ ë¡œì§ ì—ëŸ¬ ë°©ì§€
        return {d: "í™•ì¸ë¶ˆê°€" for d in sorted_dates}



==================================================
FILE_PATH: app\core\cache.py
==================================================

import json
import redis.asyncio as redis
from datetime import date, datetime
from typing import Any, Optional

# ---------------------------------------------------------
# [Redis ë° ë©”ëª¨ë¦¬ ìºì‹œ í†µí•© ê´€ë¦¬] 
# Redisê°€ ì—°ê²° ê°€ëŠ¥í•œ ìƒíƒœë©´ Redisë¥¼ ì“°ê³ , ì•„ë‹ˆë©´ ë©”ëª¨ë¦¬(Local)ë¥¼ ì”ë‹ˆë‹¤.
# ---------------------------------------------------------

# Redis ì—°ê²° ì„¤ì • (ê¸°ë³¸ê°’: localhost:6379 / DB: 0)
REDIS_URL = "redis://localhost:6379/0"
_redis_client = None
_local_cache = {} # Redis ì‹¤íŒ¨ ì‹œ ì‚¬ìš©ë  ë°±ì—… ë©”ëª¨ë¦¬ ìºì‹œ

async def get_redis():
    """Redis í´ë¼ì´ì–¸íŠ¸ ì‹±ê¸€í†¤ ë°˜í™˜"""
    global _redis_client
    if _redis_client is None:
        try:
            _redis_client = redis.from_url(REDIS_URL, decode_responses=True)
            # ì—°ê²° í™•ì¸ìš© í…ŒìŠ¤íŠ¸
            await _redis_client.ping()
            print("ğŸš€ [Redis] ì—°ê²° ì„±ê³µ (localhost:6379)")
        except Exception:
            print("âš ï¸ [Redis] ì—°ê²° ì‹¤íŒ¨! ë©”ëª¨ë¦¬ ìºì‹œ(Local) ëª¨ë“œë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
            _redis_client = False # ì—°ê²° ì‹¤íŒ¨ í‘œì‹œ
    return _redis_client

def _make_key(store_id: int, target_date: date) -> str:
    """ìºì‹œ í‚¤ ìƒì„±: 'report:1:2025-12-21'"""
    return f"report:{store_id}:{target_date.isoformat()}"



async def get_report_cache(store_id: int, target_date: date) -> Optional[dict]:
    """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ (Redis or Memory)"""
    key = _make_key(store_id, target_date)
    client = await get_redis()
    
    # 1. Redisì—ì„œ ì‹œë„
    if client:
        try:
            raw_data = await client.get(key)
            if raw_data:
                print(f"âš¡ [Redis Hit] '{key}' ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
                data = json.loads(raw_data)
                data["cached"] = True
                return data
        except Exception as e:
            print(f"âŒ [Redis Error] ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")

    # 2. Redis ì‹¤íŒ¨ ì‹œ ë©”ëª¨ë¦¬ì—ì„œ ì‹œë„
    if key in _local_cache:
        print(f"âœ… [Local Hit] '{key}' ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì—ì„œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        data = _local_cache[key]
        data["cached"] = True
        return data
        
    return None



async def set_report_cache(store_id: int, data: Any, target_date: date, ttl: int = 86400):
    """ìºì‹œì— ë°ì´í„° ì €ì¥ (Redis & Memory)"""
    key = _make_key(store_id, target_date)
    client = await get_redis()
    
    # JSON ì§ë ¬í™”
    json_data = json.dumps(data, default=str)
    
    # 1. Redis ì €ì¥
    if client:
        try:
            await client.set(key, json_data, ex=ttl)
            print(f"ğŸ’¾ [Redis Set] '{key}' ì €ì¥ ì™„ë£Œ (TTL: {ttl}s)")
        except Exception as e:
            print(f"âŒ [Redis Error] ì €ì¥ ì‹¤íŒ¨: {str(e)}")

    # 2. ë©”ëª¨ë¦¬ì—ë„ ë°±ì—… ì €ì¥
    _local_cache[key] = data
    print(f"ğŸ’¾ [Local Set] '{key}' ë©”ëª¨ë¦¬ ì €ì¥ ì™„ë£Œ")

async def get_report_object_cache(store_id: int, target_date: date) -> Optional[dict]:
    """ìºì‹œì—ì„œ 'report' í•„ë“œë§Œ ì™ ë½‘ì•„ì˜¤ê¸° (Service ê°„ê²°í™”ìš©)"""
    cached = await get_report_cache(store_id, target_date)
    return cached.get("report") if cached else None



==================================================
FILE_PATH: app\core\db.py
==================================================


from psycopg.rows import dict_row
from psycopg_pool import AsyncConnectionPool
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
pool: AsyncConnectionPool
import os
from dotenv import load_dotenv
load_dotenv()
# Local DB ì´ê±´ ì•ˆë¨
# database_url = os.getenv("DATABASE_URL", "postgresql://ai_user:1234@localhost:5432/ai_project")

# Local DB ì´ê±´ ë¨
# database_url = "postgresql://ai_user:1234@localhost:5432/ai_project"

database_url = f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@{os.getenv('DB_HOST')}:{os.getenv('DB_PORT', '5432')}/{os.getenv('DB_NAME')}"

# SQLAlchemyëŠ” "postgresql://"ë§Œ ì£¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ psycopg2ë¥¼ ì°¾ìœ¼ë¯€ë¡œ,
# ì„¤ì¹˜ëœ psycopg(v3)ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ìŠ¤í‚¤ë§ˆë¥¼ ëª…ì‹œí•´ì¤ë‹ˆë‹¤.
engine = create_engine(database_url.replace(
    "postgresql://", "postgresql+psycopg://"), echo=True)

SessionLocal = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine,
)

base = declarative_base()
# Import models directly so Base.metadata is populated for Alembic autogenerate.

# ìƒˆ ëª¨ë¸ì´ ìƒê¸°ë©´ ì•„ë˜ì— ì¶”ê°€í•˜ì„¸ìš”.

from app.menu.menu_schema import Menu  # noqa: F401
from app.user.user_schema import User  # noqa: F401
from app.store.store_schema import Store  # noqa: F401
from app.review.review_schema import Review  # noqa: F401
from app.order.order_schema import Order  # noqa: F401
from app.sales.sales_schema import SalesDaily  # noqa: F401
from app.report.report_schema import StoreReport  # noqa: F401
from app.manual.manual_schema import Manual  # noqa: F401
from app.inquiry.inquiry_schema import StoreInquiry  # noqa: F401
from app.policy.policy_schema import Policy  # noqa: F401


async def init_pool():
    global pool
    pool = AsyncConnectionPool(
        conninfo=database_url,
        kwargs={"row_factory": dict_row},
        min_size=1,
        max_size=50,
        open=False,
    )
    await pool.open()
    print("ğŸ”¥ DB pool initialized")


async def close_pool():
    global pool
    if pool:
        await pool.close()
        print("ğŸ§¹ DB pool closed")


def get_pool() -> AsyncConnectionPool:

    if pool is None:
        raise RuntimeError("DB pool is not initialized")
    return pool


async def fetch_one(sql: str, params=()) -> dict | None:
    async with pool.connection() as conn:
        async with conn.cursor() as cur:
            await cur.execute(sql, params)
            return await cur.fetchone()


async def fetch_all(sql: str, params=()) -> list[dict]:
    async with pool.connection() as conn:
        async with conn.cursor() as cur:
            await cur.execute(sql, params)
            return await cur.fetchall()


async def execute(sql: str, params=()):
    async with pool.connection() as conn:
        try:
            async with conn.cursor() as cur:
                await cur.execute(sql, params)
            await conn.commit()
        except Exception as e:
            print("execute ì‹¤í–‰ ì‹¤íŒ¨", e)
            await conn.rollback()


async def execute_return(sql: str, params=()) -> dict | None:
    async with pool.connection() as conn:
        try:
            async with conn.cursor() as cur:
                await cur.execute(sql, params)
                row = await cur.fetchone()
            await conn.commit()
            return row
        except Exception as e:
            print("execute_insert ì‹¤í–‰ ì‹¤íŒ¨", e)
            await conn.rollback()

# FastAPI Dependency Injectionìš©
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()



==================================================
FILE_PATH: app\core\response.py
==================================================

from pydantic import BaseModel
from typing import Generic, TypeVar

T = TypeVar("T")

class Response(BaseModel, Generic[T]):
  success : bool
  data : T | None = None
  message: str | None



==================================================
FILE_PATH: app\inquiry\inquiry_agent.py
==================================================

import json
from typing import Dict, Any
from langchain_core.messages import SystemMessage, HumanMessage
# [Refactoring] ë¶„ë¦¬ëœ ë…¸ë“œë“¤ Import (Clean Architecture)
from app.inquiry.inquiry_schema import InquiryState
from app.inquiry.nodes.router import router_node
from app.inquiry.nodes.sales import diagnosis_node
from app.inquiry.nodes.retrieval import manual_node, policy_node, web_search_node
from app.inquiry.nodes.answer import answer_node_v2
from app.inquiry.nodes.save import save_node
from app.clients.genai import genai_generate_text


# ===== [Phase 1] ê²€ìƒ‰ ë° ì§„ë‹¨ ì‹¤í–‰ í•¨ìˆ˜ (Entry Point) =====
async def run_search_check(store_id: int, question: str) -> Dict[str, Any]:
    """
    1ë‹¨ê³„: ì§ˆë¬¸ ë¶„ë¥˜ -> DB ê²€ìƒ‰ -> ìœ ì‚¬ë„ í‰ê°€ ê²°ê³¼ ë°˜í™˜
    """
    # 1. State ì´ˆê¸°í™”
    state = InquiryState(
        store_id=store_id,
        question=question,
        category="",
        sales_data={},
        manual_data=[],
        policy_data=[],
        final_answer="",
        inquiry_id=0,
        diagnosis_result=""
    )
    
    # 2. Router ì‹¤í–‰
    state = await router_node(state)
    category = state["category"]
    
    # 3. ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ ì‹¤í–‰
    top_doc = None
    min_dist = 1.0 
    search_results = []
    
    if category == "sales":
        # ë§¤ì¶œì€ ì‚¬ìš©ìê°€ ì„ íƒí•  í•„ìš” ì—†ì´ ë¬´ì¡°ê±´ ë°ì´í„° ë¶„ì„
        state = await diagnosis_node(state)
        min_dist = 0.0
        sales_info = state.get("sales_data", {})
        top_doc = {
            "title": "ë§¤ì¶œ ë°ì´í„° ë¶„ì„", 
            "content": sales_info.get("summary_text", "ë¶„ì„ ê²°ê³¼ ì—†ìŒ"),
            "search_params": {
                "scope": sales_info.get("scope"),
                "tables_used": sales_info.get("tables_used"),
                "period": sales_info.get("period")
            }
        }
        
    elif category == "manual":
        # ë§¤ë‰´ì–¼ ê²€ìƒ‰ ì‹¤í–‰
        state = await manual_node(state)
        docs = state.get("manual_data", [])
        meta = state.get("search_meta", {})
        min_dist = meta.get("min_distance", 1.0)
        
        if docs:
            first_line = docs[0].split("\n")[0]
            content_preview = docs[0][len(first_line)+1:]
            top_doc = {"title": first_line, "content": content_preview[:200] + "..."}
            search_results = docs 

    elif category == "policy":
        # ì •ì±… ê²€ìƒ‰ ì‹¤í–‰
        state = await policy_node(state)
        docs = state.get("policy_data", [])
        meta = state.get("search_meta", {})
        min_dist = meta.get("min_distance", 1.0)
        
        if docs:
            first_line = docs[0].split("\n")[0]
            content_preview = docs[0][len(first_line)+1:]
            top_doc = {"title": first_line, "content": content_preview[:200] + "..."}
            search_results = docs

    # [Feature] AI Contextual Check: ë¬¸ì„œ ì í•©ì„± íŒë‹¨
    recommendation = {"indices": [], "comment": ""}
    
    if search_results and category != "sales":
        try:
            # í›„ë³´êµ° ì œëª© + ì•ë¶€ë¶„ ìš”ì•½ ì¶”ì¶œ
            docs_summary = []
            for i, c in enumerate(search_results):
                lines = c.split('\n')
                title = lines[0]
                preview = lines[1][:50] + "..." if len(lines) > 1 else ""
                docs_summary.append(f"[{i}] {title} ({preview})")
            
            rec_prompt = f"""
            ì§ˆë¬¸: "{question}"
            
            ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡:
            {json.dumps(docs_summary, ensure_ascii=False, indent=2)}
            
            ìœ„ ë¬¸ì„œë“¤ì´ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°ì— 'ì¶©ë¶„íˆ ê´€ë ¨ì„±'ì´ ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.
            [Output Format]
            JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
            {{
                "relevant_indices": [0, 2],  // ê´€ë ¨ ë¬¸ì„œ ë²ˆí˜¸ (ì—†ìœ¼ë©´ [])
                "reason": "íŒë‹¨ ì´ìœ "
            }}
            """
            # ê°„ë‹¨ ì¶”ì²œ ë¡œì§ (ì¼ë‹¨ ê°„ì†Œí™”)
            rec_res = await genai_generate_text(rec_prompt)
            clean_json = rec_res.replace("```json", "").replace("```", "").strip()
            rec_data = json.loads(clean_json)
            
            relevant_indices = rec_data.get("relevant_indices", [])
            reason = rec_data.get("reason", "")
            
            if relevant_indices:
                recommendation["indices"] = relevant_indices
                recommendation["comment"] = f"âœ… AI ì¶”ì²œ: {reason}"
            else:
                recommendation["indices"] = []
                recommendation["comment"] = f"âš ï¸ AI íŒë‹¨: {reason}"
                
        except Exception as e:
            print(f"âš ï¸ ì¶”ì²œ ë¡œì§ ì—ëŸ¬: {e}")
            recommendation["comment"] = "ì¶”ì²œ ì‹œìŠ¤í…œ ì¼ì‹œ ì˜¤ë¥˜"

    return {
        "category": category,
        "min_distance": min_dist,
        "similarity_score": round((1 - min_dist) * 100, 1),
        "top_document": top_doc,
        "candidates": search_results,
        "context_data": search_results if category != "sales" else [],
        "recommendation": recommendation,
        "sales_data": state.get("sales_data", {})
    }


# ===== [Phase 2] ìµœì¢… ë‹µë³€ ìƒì„± ìŠ¤íŠ¸ë¦¬ë° (Entry Point) =====
async def run_final_answer_stream(store_id: int, question: str, category: str, mode: str, context_data: list):
    """
    2ë‹¨ê³„: ì‚¬ìš©ì ì„ íƒ(DB/Web)ì— ë”°ë¼ ë‹µë³€ ìƒì„±
    mode: 'db' (ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©) | 'web' (ì›¹ ê²€ìƒ‰ ìˆ˜í–‰)
    """
    
    yield json.dumps({"step": "init", "message": f"ğŸš€ {mode.upper()} ëª¨ë“œë¡œ ë‹µë³€ ìƒì„± ì‹œì‘..."}) + "\n"
    
    state = InquiryState(
        store_id=store_id, 
        question=question, 
        category=category,
        sales_data={}, manual_data=[], policy_data=[], final_answer="", inquiry_id=0, diagnosis_result=""
    )

    if category == "sales": # Sales Logic
        # [Optimization] Phase 1ì—ì„œ ë„˜ì–´ì˜¨ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì¬ì‚¬ìš© (LLM/DB ë¹„ìš© ì ˆê°)
        if context_data and isinstance(context_data[0], dict):
             yield json.dumps({"step": "sales", "message": "â™»ï¸ ê¸°ì¡´ ë¶„ì„ ë°ì´í„° í™œìš© ì¤‘..."}) + "\n"
             state["sales_data"] = context_data[0]
        else:
             yield json.dumps({"step": "sales", "message": "ğŸ“‰ ë§¤ì¶œ ë°ì´í„° ë¶„ì„ ì¤‘..."}) + "\n"
             state = await diagnosis_node(state)
        
        details = {
            "type": "analysis", 
            "summary": state["sales_data"].get("diagnosis_result"),
            "sales_summary": state["sales_data"].get("summary_text", "")[:100] + "..."
        }
        yield json.dumps({"step": "sales", "message": "âœ… ë¶„ì„ ì™„ë£Œ", "details": details}) + "\n"
        
    else: # Retrieval Logic
        if mode == "web":
            yield json.dumps({"step": "web_search", "message": "ğŸŒ ì™¸ë¶€ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ ì¤‘..."}) + "\n"
            state = await web_search_node(state)
            
            web_res = state["manual_data"][0] if state["manual_data"] else ""
            details = {"type": "web_result", "content": web_res}
            yield json.dumps({"step": "web_search", "message": "âœ… ì™¸ë¶€ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ", "details": details}) + "\n"
        else:
            # Context Restore
            key = "manual_data" if category == "manual" else "policy_data"
            state[key] = context_data
            yield json.dumps({"step": "check", "message": "ğŸ“š ë‚´ë¶€ DB ë°ì´í„° í™œìš©"}) + "\n"

    # Answer Generation
    yield json.dumps({"step": "answer", "message": "âœï¸ ë‹µë³€ ì‘ì„± ì¤‘..."}) + "\n"
    state = await answer_node_v2(state)
    
    # Save
    yield json.dumps({"step": "save", "message": "ğŸ’¾ ê¸°ë¡ ì €ì¥ ì¤‘..."}) + "\n"
    state = await save_node(state)
    
    yield json.dumps({
        "step": "done",
        "message": "ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
        "final_answer": state["final_answer"],
        "category": state["category"]
    }) + "\n"


==================================================
FILE_PATH: app\inquiry\inquiry_router.py
==================================================

from fastapi import APIRouter
from pydantic import BaseModel
from app.inquiry.inquiry_agent import run_search_check
from fastapi.responses import StreamingResponse
from app.inquiry.inquiry_agent import run_final_answer_stream

router = APIRouter(prefix="/inquiry", tags=["Inquiry"])


class InquiryRequest(BaseModel):
    """ì§ˆë¬¸ ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    store_id: int
    question: str

class GenerateRequest(BaseModel):
    store_id: int
    question: str
    category: str
    mode: str # 'db' or 'web'
    context_data: list = [] # DB ëª¨ë“œì¼ ë•Œ ì‚¬ìš©í•  ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸


@router.post("/check", response_model=dict)
async def check_database_search(request: InquiryRequest):
    """
    [Steps 1] DB ê²€ìƒ‰ & ìœ ì‚¬ë„ í™•ì¸ API
    ì§ˆë¬¸ì„ ë°›ì•„ ë‚´ë¶€ DB(ë§¤ë‰´ì–¼/ì •ì±…)ë¥¼ ê²€ìƒ‰í•˜ê³ , 
    ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œì™€ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. (ë‹µë³€ ìƒì„± X)
    """
    
    result = await run_search_check(request.store_id, request.question)
    return {
        "success": True,
        "data": result
    }


@router.post("/generate/stream")
async def generate_answer_stream(request: GenerateRequest):
    """
    [Steps 2] ìµœì¢… ë‹µë³€ ìƒì„± (Streaming)
    ì‚¬ìš©ìê°€ ì„ íƒí•œ ëª¨ë“œ(DB or Web)ì— ë”°ë¼ ìµœì¢… ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë°í•©ë‹ˆë‹¤.
    """
    
    return StreamingResponse(
        run_final_answer_stream(
            request.store_id, 
            request.question, 
            request.category, 
            request.mode, 
            request.context_data
        ),
        media_type="application/x-ndjson"
    )


@router.get("/history/{store_id}", response_model=list)
async def get_inquiry_history(store_id: int, limit: int = 10):
    """
    ë§¤ì¥ë³„ ì§ˆë¬¸ ì´ë ¥ ì¡°íšŒ
    
    Args:
        store_id: ë§¤ì¥ ID
        limit: ì¡°íšŒí•  ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ)
    
    Returns:
        ì§ˆë¬¸/ë‹µë³€ ì´ë ¥ ë¦¬ìŠ¤íŠ¸
    """
    from app.core.db import fetch_all
    
    query = f"""
    SELECT inquiry_id, category, question, answer, created_at
    FROM store_inquiries
    WHERE store_id = {store_id}
    ORDER BY created_at DESC
    LIMIT {limit}
    """
    
    rows = await fetch_all(query)
    
    return [
        {
            "inquiry_id": row["inquiry_id"],
            "category": row["category"],
            "question": row["question"],
            "answer": row["answer"],
            "created_at": str(row["created_at"])
        }
        for row in rows
    ]



==================================================
FILE_PATH: app\inquiry\inquiry_schema.py
==================================================

from datetime import datetime
from sqlalchemy import Column, Integer, String, Text, DateTime, ForeignKey
from sqlalchemy.sql import func
from pydantic import BaseModel
from app.core.db import base
from typing import TypedDict, List, Dict, Any

# --- LangGraph State ---
class InquiryState(TypedDict):
    """ì—ì´ì „íŠ¸ ìƒíƒœ ê´€ë¦¬"""
    store_id: int
    question: str
    category: str  # Routerê°€ ë¶„ë¥˜í•œ ì¹´í…Œê³ ë¦¬
    
    # ê° ë…¸ë“œì—ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°
    sales_data: Dict[str, Any]
    manual_data: List[str]
    policy_data: List[str]
    
    # ìµœì¢… ê²°ê³¼
    final_answer: str
    inquiry_id: int
    diagnosis_result: str # ì§„ë‹¨ ê²°ê³¼ ìš”ì•½ (ìƒˆë¡œ ì¶”ê°€)


class StoreInquiry(base):
    __tablename__ = "store_inquiries"

    inquiry_id = Column(Integer, primary_key=True, index=True)
    store_id = Column(Integer, ForeignKey("stores.store_id"), nullable=False)
    
    # ì¹´í…Œê³ ë¦¬ (ì˜ˆ: "sales", "manual", "policy") - ë¼ìš°í„°ê°€ ë¶„ë¥˜í•œ ê°’
    category = Column(String(50), nullable=False)
    
    # ì§ˆë¬¸ê³¼ ë‹µë³€ (ìƒì„¸ ë‚´ìš©ì´ë¯€ë¡œ Text íƒ€ì… ì‚¬ìš©)
    question = Column(Text, nullable=False)
    answer = Column(Text, nullable=False)
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())


# --- Pydantic Models for API ---
class InquiryCreate(BaseModel):
    store_id: int
    question: str

class InquiryResponse(BaseModel):
    inquiry_id: int
    store_id: int
    category: str
    question: str
    answer: str
    created_at: datetime

    class Config:
        from_attributes = True





==================================================
FILE_PATH: app\inquiry\inquiry_service.py
==================================================

from app.core.db import SessionLocal
from app.inquiry.inquiry_schema import StoreInquiry

def save_inquiry(store_id: int, category: str, question: str, answer: str) -> int:
    """
    ì§ˆë¬¸ê³¼ AI ë‹µë³€ì„ DBì— ì €ì¥
    
    Args:
        store_id: ë§¤ì¥ ID
        category: ì§ˆë¬¸ ì¹´í…Œê³ ë¦¬ (sales/manual/policy)
        question: ì§ˆë¬¸ ë‚´ìš©
        answer: AI ë‹µë³€
    
    Returns:
        ìƒì„±ëœ inquiry_id
    """
    db = SessionLocal()
    try:
        new_inquiry = StoreInquiry(
            store_id=store_id,
            category=category,
            question=question,
            answer=answer
        )
        db.add(new_inquiry)
        db.commit()
        db.refresh(new_inquiry)
        return new_inquiry.inquiry_id
    finally:
        db.close()



==================================================
FILE_PATH: app\inquiry\nodes\answer.py
==================================================

import json
from datetime import datetime, date
from typing import Dict, Any, List

# External App Imports
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI
from app.inquiry.inquiry_schema import InquiryState

# ===== Step 6: Answer Synthesis Node (ë‹µë³€ ìƒì„± - Analytical) =====
async def answer_node_v2(state: InquiryState) -> InquiryState:
    """ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'í‘œ(Table)' ì¤‘ì‹¬ì˜ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
    question = state["question"]
    category = state["category"]
    
    # 1. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    context_text = ""
    if category == "sales":
        if "sales_data" in state and state["sales_data"]:
             context_text = state["sales_data"].get("summary_text", "")
    else:
        # manual / policy ë°ì´í„° í†µí•©
        docs = state.get("manual_data", []) + state.get("policy_data", [])
        context_text = "\n\n".join(docs)
    
    # 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (Markdown Table ê°•ì œ -> ìƒí™©ì— ë”°ë¼ ìœ ì—°í•˜ê²Œ)
    system_prompt = (
        "ë‹¹ì‹ ì€ í”„ëœì°¨ì´ì¦ˆ ìˆ˜ì„ ë°ì´í„° ë¶„ì„ê°€(Chief Analyst)ì…ë‹ˆë‹¤. "
        "ì œê³µëœ [ë¶„ì„ìš© ë°ì´í„°]ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŒ©íŠ¸ì— ì…ê°í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”.\n\n"
        
        "[ì‘ì„± ê·œì¹™ - Strict Rules]\n"
        "1. **Reference Citation (ì¶œì²˜ ëª…ì‹œ)**: ë‹µë³€ ì‹œ ë°˜ë“œì‹œ **ì°¸ê³ í•œ ë§¤ë‰´ì–¼/ê·œì •ì˜ ì œëª©**ê³¼ í•µì‹¬ ë‚´ìš©ì„ ì¸ìš©í•´ì„œ ë‹µë³€í•˜ì„¸ìš”. ì˜ˆ: 'ì°¸ê³ í•˜ì‹  [í™˜ë¶ˆ ê·œì • ê°€ì´ë“œ]ì— ë”°ë¥´ë©´...'\n"
        "2. **Evidence Based**: [ë¶„ì„ìš© ë°ì´í„°]ì— ìˆëŠ” ë‚´ìš©ì„ ìµœìš°ì„ ìœ¼ë¡œ ê·¼ê±°ë¡œ ì‚¼ìœ¼ì„¸ìš”. ìœ ì‚¬ë„ê°€ ë†’ê²Œ ë‚˜ì˜¨ ë¬¸ì„œê°€ ìˆë‹¤ë©´ í•´ë‹¹ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ êµ¬ì„±í•˜ì„¸ìš”.\n"
        "3. **Markdown Table í•„ìˆ˜**: Best/Worst ë©”ë‰´, ì§€ì  ë¹„êµ ë“± ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ë°ì´í„°ëŠ” **ë°˜ë“œì‹œ Markdown í‘œ(Table)**ë¡œ ì‘ì„±í•˜ì—¬ ê°€ë…ì„±ì„ ë†’ì´ì„¸ìš”. (ì»¬ëŸ¼ ì˜ˆ: ìˆœìœ„, ë©”ë‰´ëª…, íŒë§¤ëŸ‰, ë§¤ì¶œì•¡ê°€, ë¦¬ë·° ìš”ì•½)\n"
        "4. **í™”í ë‹¨ìœ„**: ë°˜ë“œì‹œ **ì›(KRW)**ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n"
        "5. **ì›ì¸ ë¶„ì„**: ì¶”ì¸¡ì´ ì•„ë‹ˆë¼ ë°ì´í„°ì— ê·¼ê±°í•œ ë¶„ì„ë§Œ ìˆ˜í–‰í•˜ì„¸ìš”."
    )
    
    # ë©”ì‹œì§€ êµ¬ì„±
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ì§ˆë¬¸: {question}\n\n[ë¶„ì„ìš© ë°ì´í„°]\n{context_text}")
    ]
    
    # 3. LLM í˜¸ì¶œ
    llm = ChatOpenAI(model="gpt-4o", temperature=0)
    response = await llm.ainvoke(messages)
    
    # 4. ê²°ê³¼ ì €ì¥ (Structured JSON ìƒì„±)
    # UIê°€ ì°¨íŠ¸, ë©”íŠ¸ë¦­, ë¦¬ë·° ê·¼ê±°ë¥¼ ë Œë”ë§í•  ìˆ˜ ìˆë„ë¡ JSON êµ¬ì¡°í™”
    final_output = {
        "answer": response.content,
        "category": category
    }
    
    if category == "sales" and "sales_data" in state:
        sd = state["sales_data"]
        final_output["chart_data"] = sd.get("chart_data")
        final_output["chart_setup"] = sd.get("chart_setup")
        final_output["key_metrics"] = sd.get("key_metrics")
        
        # [Evidence] ë¶„ì„ì— ì‚¬ìš©ëœ ë¦¬ë·° ë°ì´í„° ì „ë‹¬ (ë©”ë‰´ë³„ + ì „ì²´ ìµœì‹ )
        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ í•©ì¹˜ê¸°
        all_reviews = sd.get("recent_reviews", []) + sd.get("menu_specific_reviews", [])
        # ê°„ë‹¨í•œ ì¤‘ë³µ ì œê±° (ë‚´ìš© ê¸°ì¤€)
        seen = set()
        unique_reviews = []
        for r in all_reviews:
            if r.get('review_text') and r['review_text'] not in seen:
                seen.add(r['review_text'])
                unique_reviews.append(r)
                
        final_output["used_reviews"] = unique_reviews
        
        # UIëŠ” 'summary' í‚¤ê°€ ì—†ìœ¼ë©´ 'answer'ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì¶œë ¥í•˜ì§€ ì•ŠìŒ? 
        # detailì— ë‹µë³€ ë‚´ìš© ì €ì¥
        final_output["detail"] = response.content
    else:
        final_output["detail"] = response.content

    def json_serial(obj):
        if isinstance(obj, (datetime, date)):
            return obj.isoformat()
        raise TypeError(f"Type {type(obj)} not serializable")

    state["final_answer"] = json.dumps(final_output, ensure_ascii=False, default=json_serial)
    
    print(f"âœ… [Analyst Answer] ë¶„ì„ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ (Structured)")
    return state



==================================================
FILE_PATH: app\inquiry\nodes\retrieval.py
==================================================

import json
from typing import Dict, Any, List

# External App Imports
from app.clients.genai import genai_generate_with_grounding
from app.core.db import fetch_all
from app.inquiry.inquiry_schema import InquiryState

# ===== Step 4: Manual RAG Node (ë§¤ë‰´ì–¼ ê²€ìƒ‰) =====
async def manual_node(state: InquiryState) -> InquiryState:
    """ë§¤ë‰´ì–¼ DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (Vector Search)"""
    if state["category"] != "manual":
        return state
    
    question = state["question"]
    
    # OpenAI Embeddingsë¡œ ì§ˆë¬¸ ë²¡í„°í™”
    from langchain_openai import OpenAIEmbeddings
    embeddings_model = OpenAIEmbeddings(model="text-embedding-3-small")
    question_vector = embeddings_model.embed_query(question)
    
    # pgvector ìœ ì‚¬ë„ ê²€ìƒ‰ (ì½”ì‚¬ì¸ ê±°ë¦¬ ê¸°ì¤€ Top 3)
    # distanceê°€ 0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬í•¨
    query = f"""
    SELECT title, content, category,
           embedding <=> '{question_vector}'::vector AS distance
    FROM manuals
    ORDER BY distance
    LIMIT 5
    """
    
    rows = await fetch_all(query)
    
    # ê²€ìƒ‰ ê²°ê³¼ ë° ìµœì†Œ ê±°ë¦¬ ì €ì¥
    min_distance = 1.0 # ê¸°ë³¸ê°’ (ë¶ˆì¼ì¹˜)
    if rows:
        min_distance = min([r['distance'] for r in rows])
        
    state["manual_data"] = [
        f"[{row['title']}] (ìœ ì‚¬ë„: {1 - row['distance']:.2f})\n{row['content']}"
        for row in rows
    ]
    
    if "search_meta" not in state: state["search_meta"] = {}
    state["search_meta"] = {"min_distance": min_distance, "source": "manual_db"}
    
    print(f"ğŸ“– [Manual] ê²€ìƒ‰ ì™„ë£Œ (Min Distance: {min_distance:.4f})")
    return state


# ===== Step 5: Policy RAG Node (ì •ì±… ê²€ìƒ‰) =====
async def policy_node(state: InquiryState) -> InquiryState:
    """ìš´ì˜ ì •ì±… ë§¤ë‰´ì–¼ ê²€ìƒ‰ (Policies í…Œì´ë¸” ì¡°íšŒ)"""
    if state["category"] != "policy":
        return state
    
    question = state["question"]
    
    from langchain_openai import OpenAIEmbeddings
    embeddings_model = OpenAIEmbeddings(model="text-embedding-3-small")
    question_vector = embeddings_model.embed_query(question)
    
    query = f"""
    SELECT title, content, category,
           embedding <=> '{question_vector}'::vector AS distance
    FROM policies
    ORDER BY distance
    LIMIT 5
    """
    
    rows = await fetch_all(query)
    
    min_distance = 1.0
    if rows:
        min_distance = min([r['distance'] for r in rows])
        
    state["policy_data"] = [
        f"[{row['title']}] (ìœ ì‚¬ë„: {1 - row['distance']:.2f})\n{row['content']}"
        for row in rows
    ]
    
    state["search_meta"] = {"min_distance": min_distance, "source": "policy_db"}
    
    print(f"ğŸ“œ [Policy] ê²€ìƒ‰ ì™„ë£Œ (Min Distance: {min_distance:.4f})")
    return state


# ===== Step 5.5: Web Search Node (ì™¸ë¶€ ê²€ìƒ‰ - Google Grounding) =====
async def web_search_node(state: InquiryState) -> InquiryState:
    """ë‚´ë¶€ DB ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ì™¸ë¶€ ì›¹ ê²€ìƒ‰ ìˆ˜í–‰ (Google Gemini Grounding)"""
    question = state["question"]
    print(f"ğŸŒ [Google Grounding] ë‚´ë¶€ ë¬¸ì„œ ë¶€ì¡± -> êµ¬ê¸€ ê²€ìƒ‰ ìˆ˜í–‰: {question}")
    
    try:
        # êµ¬ê¸€ ê²€ìƒ‰ Groundingì„ í†µí•œ ë‹µë³€ ìƒì„±
        grounded_response = await genai_generate_with_grounding(question)
        
        # ê²°ê³¼ ì €ì¥
        state["manual_data"] = [f"[Google ê²€ìƒ‰ ê²°ê³¼ ê¸°ë°˜ ë‹µë³€]\n{grounded_response}"]
        state["search_meta"] = {"source": "web_search", "min_distance": 0.0}
        
    except Exception as e:
        print(f"âŒ Google Grounding ì‹¤íŒ¨: {e}")
        state["manual_data"] = [f"ì™¸ë¶€ ê²€ìƒ‰ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (Error: {str(e)})"]
        
    return state



==================================================
FILE_PATH: app\inquiry\nodes\router.py
==================================================

import json
from datetime import datetime, date
from typing import Dict, Any, List

# External App Imports
from app.clients.genai import genai_generate_text
from app.inquiry.inquiry_schema import InquiryState

# ===== Router Node (ì§ˆë¬¸ ë¶„ë¥˜) =====
async def router_node(state: InquiryState) -> InquiryState:
    """
    ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
    - sales: ë§¤ì¶œ, ì„±ê³¼, í†µê³„ ê´€ë ¨
    - manual: ê¸°ê¸° ì‚¬ìš©ë²•, ë ˆì‹œí”¼, ê¸°ìˆ  ì§€ì›
    - policy: ìš´ì˜ ê·œì •, ê³ ê° ì‘ëŒ€, ë³¸ì‚¬ ì •ì±…
    """
    question = state["question"]
    
    prompt = f"""
    ë‹¹ì‹ ì€ í”„ëœì°¨ì´ì¦ˆ ë§¤ì¥ ì§ˆë¬¸ ë¶„ë¥˜ AIì…ë‹ˆë‹¤. 
    ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ë¥¼ íŒŒì•…í•˜ì—¬ ë‹¤ìŒ 3ê°€ì§€ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.

    ì§ˆë¬¸: "{question}"

    1. sales (ë§¤ì¶œ/ë°ì´í„°):
       - ë§¤ì¶œ, íŒë§¤ëŸ‰, ì£¼ë¬¸ ê±´ìˆ˜, ë©”ë‰´ë³„ ì„±ê³¼, í†µê³„
       - "ì§€ë‚œì£¼ ë§¤ì¶œ ì–´ë•Œ?", "ê°€ì¥ ë§ì´ íŒ”ë¦° ë©”ë‰´ëŠ”?"

    2. manual (ë§¤ë‰´ì–¼/ê¸°ìˆ ):
       - ê¸°ê¸° ì¡°ì‘, ê³ ì¥ ìˆ˜ë¦¬, ì²­ì†Œ ë°©ë²•, ë ˆì‹œí”¼
       - "ì»¤í”¼ë¨¸ì‹  ì²­ì†Œ ì–´ë–»ê²Œ í•´?", "ì™€ì´íŒŒì´ ì—°ê²°ë²•"

    3. policy (ì •ì±…/ì™¸ë¶€ì •ë³´):
       - ë§¤ì¥ ìš´ì˜ ê·œì •, í™˜ë¶ˆ/ë°˜í’ˆ ì •ì±…, ê³ ê° ì‘ëŒ€ ë§¤ë‰´ì–¼
       - **[ì¤‘ìš”]**: "ë§›ì§‘ ì¶”ì²œ", "ë‚ ì”¨", "ë‰´ìŠ¤", "ì£¼ë³€ ìƒê¶Œ" ë“± ì™¸ë¶€ ì •ë³´ ê²€ìƒ‰ì´ í•„ìš”í•œ ê²½ìš°ë„ 'policy'ë¡œ ë¶„ë¥˜

    [Output Format]
    JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”:
    {{"category": "sales" | "manual" | "policy", "reason": "ë¶„ë¥˜ ì´ìœ "}}
    """ 
    
    # LLM í˜¸ì¶œ (Geminië¡œ ê°„ì†Œí™”)
    try:
        # ê°€ë³ê³  ë¹ ë¥¸ gemai ì‚¬ìš©
        response = await genai_generate_text(prompt)
        
        # JSON íŒŒì‹±
        content = response.replace("```json", "").replace("```", "").strip()
        data = json.loads(content)
        category = data.get("category", "policy") # ê¸°ë³¸ê°’ policy
        reason = data.get("reason", "")
    except Exception as e:
        print(f"âš ï¸ [Router] ë¶„ë¥˜ ì˜¤ë¥˜ (Fallback to policy): {e}")
        category = "policy"
        reason = "Error Parsing"
        data = {}

    print(f"ğŸ”€ [Router] Category Decision: {category} (Reason: {reason})")
    
    # State ì—…ë°ì´íŠ¸
    state["category"] = category
    return state



==================================================
FILE_PATH: app\inquiry\nodes\sales.py
==================================================

import json
from datetime import datetime, timedelta
from typing import Dict, Any, List

# External App Imports
from app.clients.genai import genai_generate_text
from app.core.db import fetch_all
from app.inquiry.inquiry_schema import InquiryState

# ===== Search Param Extraction Helper =====
async def extract_search_params(question: str):
    """
    ì§ˆë¬¸ ë¶„ì„ -> ë¶„ì„ ëŒ€ìƒ(ë§¤ì¥ë“¤) & í•„ìš”í•œ ë°ì´í„° ì†ŒìŠ¤(í…Œì´ë¸”) ê²°ì •
    """
    prompt = f"""
    ë‹¹ì‹ ì€ ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ 'ìˆëŠ” ê·¸ëŒ€ë¡œ ì¶”ì¶œ'í•˜ëŠ” AIì…ë‹ˆë‹¤. (ë²ˆì—­/í•´ì„ ê¸ˆì§€)
    ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë¶„ì„ ëŒ€ìƒ ë§¤ì¥ì™€ í•„ìš”í•œ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.
    
    ì§ˆë¬¸: "{question}"
    
    [ì¶”ì¶œ ê·œì¹™]
    
    1. target_store_codes: ë¶„ì„ ëŒ€ìƒ ë§¤ì¥ëª… (í•œê¸€ í‚¤ì›Œë“œ)
       - âŒ ì ˆëŒ€ ì˜ì–´ë¡œ ë²ˆì—­í•˜ì§€ ë§ˆì„¸ìš”. (No English Codes like 'SEOUL_GANGNAM')
       - ì§ˆë¬¸ì— ìˆëŠ” ë‹¨ì–´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
       - "ê°•ë‚¨ì  ë§¤ì¶œ" -> ["ê°•ë‚¨"]
       - "ì„œìš¸ì´ë‘ ë¶€ì‚° ë¹„êµ" -> ["ì„œìš¸", "ë¶€ì‚°"]
       - "ì „ì²´", "ëª¨ë“ " -> ["ALL"]
       
    2. required_tables: ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ì¡°íšŒí•´ì•¼ í•  í…Œì´ë¸” ë¦¬ìŠ¤íŠ¸ (ë³µìˆ˜ ì„ íƒ ê°€ëŠ¥)
       - "orders": ë©”ë‰´ íŒë§¤ëŸ‰, ì¸ê¸°/ë¹„ì¸ê¸° ë©”ë‰´ ì‹ë³„ (What)
       - "sales_daily": ë§¤ì¶œ ì¶”ì´, ë‚ ì”¨ ì •ë³´ í¬í•¨ (External Factor)
       - "reviews": íŒë§¤/ë§¤ì¶œì˜ 'ì›ì¸(Why)' ë¶„ì„, ê³ ê° ë°˜ì‘, ë§› í‰ê°€ (ì´ìœ /ë¶„ì„ ìš”ì²­ ì‹œ í•„ìˆ˜ í¬í•¨)
       
    [í…Œì´ë¸” ì„ íƒ ê°€ì´ë“œ]
    - "ì™œ ë§¤ì¶œì´ ì¤„ì—ˆì–´?" -> ["sales_daily", "reviews"] (ì¶”ì´ + ì›ì¸)
    - "ì•ˆ íŒ”ë¦° ë©”ë‰´ì™€ ì´ìœ " -> ["orders", "reviews"] (ë©”ë‰´ + ì›ì¸)
    - "ê·¸ëƒ¥ ë§¤ì¶œ ë³´ì—¬ì¤˜" -> ["sales_daily"]
       
    [ì¶œë ¥ ì˜ˆì‹œ]
    {{
        "target_store_codes": ["ê°•ë‚¨"], 
        "required_tables": ["sales_daily", "reviews"],
        "reason": "ê°•ë‚¨ì ì˜ ë§¤ì¶œ ì¶”ì´ì™€ ë¦¬ë·°ë¥¼ ë¶„ì„í•˜ê¸° ìœ„í•¨"
    }}
    """
    try:
        response = await genai_generate_text(prompt)
        clean_text = response.replace("```json", "").replace("```", "").strip()
        parsed = json.loads(clean_text)
        return parsed
    except:
        return {"target_store_codes": ["ALL"], "required_tables": ["sales_daily", "orders"], "reason": "Error parsing"}

# ===== Step 3: Diagnosis Node (Multi-Store Support) =====
async def diagnosis_node(state: InquiryState) -> InquiryState:
    """
    [Sales Analysis V2] 
    1. ë§¤ì¥ Scope í™•ì¸ (ì„œìš¸/ë¶€ì‚°/ê°•ì›/ì „ì²´)
    2. ìµœê·¼ ë°ì´í„° ê¸°ì¤€ì¼(Anchor Date) ì‚°ì¶œ
    3. í•„ìš”í•œ í…Œì´ë¸”ë§Œ ê³¨ë¼ì„œ ë™ì  ì¿¼ë¦¬ (Orders / SalesDaily / Reviews)
    """
    if state["category"] != "sales":
        return state
        
    print(f"ğŸ•µï¸â€â™€ï¸ [Diagnosis V2] ë¶„ì„ ì‹œì‘: {state['question']}")
    
    # 1. ê²€ìƒ‰ íŒŒë¼ë¯¸í„° ì¶”ì¶œ (LLM)
    search_params = await extract_search_params(state['question'])
    
    target_store_codes = search_params.get("target_store_codes", ["ALL"])
    required_tables = search_params.get("required_tables", [])
    date_range_str = search_params.get("date_range", "DATE(o.ordered_at) >= DATE('now', '-7 days')")
    reason = search_params.get("reason", "")
    
    print(f"   ğŸ¯ íƒ€ê²Ÿ(List): {target_store_codes}, Tables: {required_tables}")
    
    # Store ID Mapping
    collected_data = {
        "scope": ", ".join(target_store_codes),
        "tables_used": required_tables,
        "period": "ìµœê·¼ 7ì¼ (ìë™ ì„¤ì •)" if "7 days" in date_range_str else "ì‚¬ìš©ì ì§€ì •",
        "reason": reason
    }
    
    try:
        # DB ì—°ê²° ë° ìŠ¤í† ì–´ ID ì¡°íšŒ (ê³µí†µ)
        store_codes = []
        target_ids = []
        # target_store_id = None # ë‹¨ì¼ ìŠ¤í† ì–´ìš© (ë¹„ì „ìš©)

        q_stores = "SELECT store_id, store_name, region FROM stores"
        all_stores = await fetch_all(q_stores)
        print(f"ğŸ•µï¸ [Debug] DB Stores: {all_stores}") # ì‹¤ì œ DBì— ì–´ë–»ê²Œ ì €ì¥ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        
        # Scope Resolution (AI-Powered Matching)
        if "ALL" in target_store_codes:
            store_codes = [s['store_name'] for s in all_stores]
            target_ids = [s['store_id'] for s in all_stores]
        else:
            # [AI Matcher] ë‹¨ìˆœ ë¬¸ìì—´ ë¹„êµ ëŒ€ì‹  LLMì´ íŒë‹¨ (í•œê¸€/ì˜ì–´/ë³„ì¹­ ì™„ë²½ ëŒ€ì‘)
            match_prompt = f"""
            ë‹¹ì‹ ì€ ë°ì´í„° ë§¤ì¹­ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
            ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ 'í‚¤ì›Œë“œ'ì™€ ì‹¤ì œ DBì— ìˆëŠ” 'ë§¤ì¥ ëª©ë¡'ì„ ë³´ê³ , ì˜ë„ì— ë§ëŠ” ë§¤ì¥ì˜ IDë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.

            1. ì‚¬ìš©ì í‚¤ì›Œë“œ: {target_store_codes}
            2. DB ë§¤ì¥ ëª©ë¡: {json.dumps(all_stores, ensure_ascii=False)}

            [ë§¤ì¹­ ê·œì¹™]
            - "ê°•ë‚¨" -> "ì„œìš¸ ê°•ë‚¨ì " (O)
            - "SEOUL" -> "ì„œìš¸ ê°•ë‚¨ì " (O)
            - "ë³¸ì " -> "ì„œìš¸ ê°•ë‚¨ì " (ë§Œì•½ ê°•ë‚¨ì´ ë³¸ì ì´ë¼ë©´ ë¬¸ë§¥ìƒ íŒë‹¨, ë¶ˆí™•ì‹¤í•˜ë©´ Skip)
            - "ì†ì´ˆ" -> "ê°•ì› ì†ì´ˆì " (O)
            
            [Output JSON]
            ë°˜ë“œì‹œ ë§¤ì¹­ëœ store_id ë¦¬ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•˜ì„¸ìš”.
            {{"matched_ids": [1, 3]}}
            """
            try:
                m_res = await genai_generate_text(match_prompt)
                m_clean = m_res.replace("```json", "").replace("```", "").strip()
                m_data = json.loads(m_clean)
                target_ids = m_data.get("matched_ids", [])
                
                # ë§¤ì¹­ëœ IDë¡œ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ì—­ì¶”ì 
                store_codes = [s['store_name'] for s in all_stores if s['store_id'] in target_ids]
                print(f"ğŸ¤– [AI Matcher] Mapped {target_store_codes} -> IDs: {target_ids} ({store_codes})")
                
            except Exception as e:
                print(f"âš ï¸ [AI Matcher] Error: {e}")
                # Fallback: ê¸°ì¡´ ë‹¨ìˆœ ë§¤ì¹­ (ì•ˆì „ì¥ì¹˜)
                for code in target_store_codes:
                    clean_code = code.replace(" ", "").strip()
                    for s in all_stores:
                        if clean_code and (clean_code in s['store_name'].replace(" ", "") or clean_code in (s['region'] or "")):
                             if s['store_id'] not in target_ids:
                                 target_ids.append(s['store_id'])
                                 store_codes.append(s['store_name'])
                            
        # [UI Fix] ì‹¤ì œ ë§¤ì¹­ëœ ë§¤ì¥ëª… ì „ë‹¬ (ì¤‘ìš”)
        if store_codes:
            collected_data["target_store_name"] = ", ".join(store_codes)
        else:
            collected_data["target_store_name"] = "ì „ì²´ ì§€ì  (ì‹ë³„ ì‹¤íŒ¨)" if "ALL" not in target_store_codes else "ì „ì²´ ì§€ì "
        
        # [Anchor Date Fix] ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ì‹¤ì œ ë§ˆì§€ë§‰ ë‚ ì§œ í™•ì¸
        # í˜„ì¬ ì‹œìŠ¤í…œ ì‹œê°„(2026ë…„)ê³¼ ë°ì´í„° ì‹œê°„(2025ë…„) ë¶ˆì¼ì¹˜ í•´ê²°
        anchor_date = None
        q_max_date = "SELECT MAX(sale_date) as last_date FROM sales_daily"
        if target_ids:
             ids_str = ",".join(map(str, target_ids))
             q_max_date += f" WHERE store_id IN ({ids_str})"
             
        try:
            date_rows = await fetch_all(q_max_date)
            if date_rows and date_rows[0]['last_date']:
                anchor_date = date_rows[0]['last_date']
                
                # date ê°ì²´ í™•ì¸ ë° ë³€í™˜
                if isinstance(anchor_date, str):
                    curr_date = datetime.strptime(anchor_date, "%Y-%m-%d").date()
                else:
                    curr_date = anchor_date
                    
                start_date = curr_date - timedelta(days=6) # 1ì£¼ì¼
                # [CRITICAL FIX] Postgres í˜¸í™˜ì„ ìœ„í•´ ëª…ì‹œì  ë‚ ì§œ ë¬¸ìì—´ ì‚¬ìš©
                date_range_str = f"'{start_date}' AND '{curr_date}'"
                print(f"ğŸ“… [Smart Period] ë°ì´í„° ê¸°ë°˜ ê¸°ê°„ ì¬ì„¤ì •: {start_date} ~ {curr_date}")
            else:
                print("âš ï¸ [Smart Period] ë°ì´í„°ê°€ ì—†ì–´ ê¸°ë³¸ ê¸°ê°„(ìµœê·¼ 7ì¼) ì‚¬ìš©")
                # Fallback: Postgres Syntax
                date_range_str = "CURRENT_DATE - INTERVAL '7 days' AND CURRENT_DATE"
        except Exception as e:
            print(f"âš ï¸ [Smart Period] Error: {e}")
            
        print(f"ğŸ” [Diagnosis] Effective Date Range: {date_range_str}")

        # (A) Sales Daily (ë§¤ì¶œ ì¶”ì´)
        if "sales_daily" in required_tables:
            where_sql = f"DATE(s.sale_date) BETWEEN {date_range_str}"
            if target_ids:
                ids_str = ",".join(map(str, target_ids))
                where_sql += f" AND s.store_id IN ({ids_str})"
            
            q_sales = f"""
                SELECT s.sale_date, st.store_name, SUM(s.total_sales) as total_sales, SUM(s.total_orders) as total_orders, MAX(s.weather_info) as weather_info
                FROM sales_daily s
                JOIN stores st ON s.store_id = st.store_id
                WHERE {where_sql}
                GROUP BY s.sale_date, st.store_name
                ORDER BY s.sale_date ASC
            """
            rows = await fetch_all(q_sales)
            collected_data["daily_trend"] = rows
            
            # Chart Data
            chart_data = []
            for r in rows:
                chart_data.append({
                    "date": r['sale_date'],
                    "store": r['store_name'],
                    "sales": float(r['total_sales']) if r['total_sales'] else 0,
                    "orders": int(r['total_orders']) if r['total_orders'] else 0
                })
            collected_data["chart_data"] = chart_data

        # (B) Orders (ë©”ë‰´ ë¶„ì„)
        # [Safety Lock] ë©”ë‰´ ë¶„ì„(Orders) ì‹œ ë¦¬ë·° ê°•ì œ ì¶”ê°€
        if "orders" in required_tables:
            if "reviews" not in required_tables:
                print("âš ï¸ [Auto-Fix] ë©”ë‰´ ë¶„ì„ì„ ìœ„í•´ Reviews í…Œì´ë¸” ê°•ì œ ì¶”ê°€")
                required_tables.append("reviews")
                
            where_sql = f"DATE(o.ordered_at) BETWEEN {date_range_str}"
            if target_ids:
                 ids_str = ",".join(map(str, target_ids))
                 where_sql += f" AND o.store_id IN ({ids_str})"
            
            q_menu = f"""
                SELECT 
                    m.menu_id,
                    m.menu_name, 
                    m.category, 
                    SUM(o.quantity) as qty, 
                    SUM(o.total_price) as rev
                FROM orders o
                JOIN menus m ON o.menu_id = m.menu_id
                WHERE {where_sql}
                GROUP BY m.menu_id, m.menu_name, m.category
                ORDER BY qty DESC
                LIMIT 5
            """
            # 1. Top 5 Fetch
            rows_top = await fetch_all(q_menu)
            print(f"ğŸ“Š [Diagnosis] Top Menus Fetched: {len(rows_top)}")
            
            # 2. Worst 5 Fetch
            q_worst = q_menu.replace("DESC", "ASC").replace("LIMIT 5", "LIMIT 5")
            rows_worst = await fetch_all(q_worst)
            print(f"ğŸ“Š [Diagnosis] Worst Menus Fetched: {len(rows_worst)}")
            
            # 3. Review Binding Logic
            all_target_menus = rows_top + rows_worst
            target_menu_ids = [r['menu_id'] for r in all_target_menus]
            
            menu_review_map = {} 
            
            if target_menu_ids:
                 ids_str_menu = ",".join(map(str, set(target_menu_ids)))
                 q_deep = f"""
                    SELECT o.menu_id, r.rating, r.review_text, o.ordered_at
                    FROM reviews r
                    JOIN orders o ON r.order_id = o.order_id
                    WHERE o.menu_id IN ({ids_str_menu}) 
                    AND DATE(o.ordered_at) BETWEEN {date_range_str}
                 """
                 
                 # [Critial Fix] ì§€ì  í•„í„°ë§ ëˆ„ë½ ìˆ˜ì •
                 if target_ids:
                     ids_str_store = ",".join(map(str, target_ids))
                     q_deep += f" AND o.store_id IN ({ids_str_store})"
                     
                 q_deep += " ORDER BY r.created_at DESC"
                 deep_reviews = await fetch_all(q_deep)
                 print(f"ğŸ’¬ [Diagnosis] Bound Reviews Fetched: {len(deep_reviews)}")
                 
                 # UI ì¦ê±°ìš© ì €ì¥
                 collected_data["menu_specific_reviews"] = deep_reviews
                 
                 for dr in deep_reviews:
                     mid = dr['menu_id']
                     if mid not in menu_review_map:
                         menu_review_map[mid] = []
                     menu_review_map[mid].append(f"â­{dr['rating']}: {dr['review_text']}")
            
            # 4. Attach to Menu Data
            for r in rows_top:
                r['related_reviews'] = menu_review_map.get(r['menu_id'], [])[:10]
            for r in rows_worst:
                r['related_reviews'] = menu_review_map.get(r['menu_id'], [])[:10]

            collected_data["top_selling_menus"] = rows_top
            collected_data["low_selling_menus"] = rows_worst

        # (C) Reviews (ì¼ë°˜ ì¡°íšŒ)
        if "reviews" in required_tables:
            # Join with orders to get date & store filtering
            where_sql = f"DATE(o.ordered_at) BETWEEN {date_range_str}"
            if target_ids:
                ids_str = ",".join(map(str, target_ids))
                where_sql += f" AND o.store_id IN ({ids_str})"
            
            q_review = f"""
                SELECT s.store_name, r.rating, r.review_text, o.ordered_at
                FROM reviews r
                JOIN orders o ON r.order_id = o.order_id
                JOIN stores s ON o.store_id = s.store_id
                WHERE {where_sql}
                ORDER BY o.ordered_at DESC
                LIMIT 500
            """
            rows = await fetch_all(q_review)
            collected_data["recent_reviews"] = rows
            print(f"ğŸ’¬ [Diagnosis] Recent Reviews Fetched: {len(rows)}")

    except Exception as e:
        print(f"âŒ [Diagnosis] Critical Error: {e}")
        collected_data["error"] = str(e)
        collected_data["summary_text"] = f"ë°ì´í„° ì¡°íšŒ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
        
    # 3. Summary Generation (LLMì„ ìœ„í•œ ìš”ì•½ í…ìŠ¤íŠ¸)
    # [Contextual Binding] ë©”ë‰´ì™€ ë¦¬ë·°ë¥¼ í•¨ê»˜ ì œê³µ
    summary_text = f"=== ğŸ“Š ë¶„ì„ ë¦¬í¬íŠ¸ ({', '.join(store_codes)}) ===\n"
    summary_text += f"ê¸°ê°„: {date_range_str}\n\n"
    
    if "daily_trend" in collected_data:
        summary_text += "[ì¼ë³„ ë§¤ì¶œ ë°ì´í„° (ì§€ì ë³„ êµ¬ë¶„)]\n"
        for r in collected_data["daily_trend"]:
            sales_val = float(r['total_sales']) if r['total_sales'] else 0
            weather = r.get('weather_info', '-')
            summary_text += f"- [{r['sale_date']}] {r['store_name']}: {sales_val:,.0f}ì› (ì£¼ë¬¸ {r['total_orders']}ê±´, ë‚ ì”¨ {weather})\n"

    if "top_selling_menus" in collected_data:
        summary_text += "\n[í†µí•© ì¸ê¸° ë©”ë‰´ Top 5 (Best)]\n"
        for m in collected_data["top_selling_menus"]:
            summary_text += f"- {m['menu_name']} ({m['category']}): {m['qty']}ê°œ íŒë§¤, {int(m['rev']):,}ì›\n"
            if 'related_reviews' in m and m['related_reviews']:
                reviews_str = " / ".join(m['related_reviews'])
                summary_text += f"  (ğŸ” ê³ ê° ë¦¬ë·°: {reviews_str})\n"

    if "low_selling_menus" in collected_data:
        summary_text += "\n[í†µí•© íŒë§¤ ì €ì¡° ë©”ë‰´ Top 5 (Worst)]\n"
        for m in collected_data["low_selling_menus"]:
            summary_text += f"- {m['menu_name']} ({m['category']}): {m['qty']}ê°œ íŒë§¤, {int(m['rev']):,}ì›\n"
            if 'related_reviews' in m and m['related_reviews']:
                reviews_str = " / ".join(m['related_reviews'])
                summary_text += f"  (ğŸ” ê³ ê° ë¦¬ë·°: {reviews_str})\n"
                
    if "recent_reviews" in collected_data and isinstance(collected_data["recent_reviews"], list):
        summary_text += "\n[ìµœê·¼ ê³ ê° ë¦¬ë·° ë°ì´í„° (ë§¤ì¥ ì „ì²´)]\n"
        for r in collected_data["recent_reviews"][:20]: # ìƒìœ„ 20ê°œë§Œ
            s_name = r.get('store_name', '')
            summary_text += f"- [{s_name}] â­{r.get('rating')}: {r.get('review_text')}\n"

    collected_data["summary_text"] = summary_text
    
    # 5. Result for Chat UI Chart (Chart Data Formatting)
    if "daily_trend" in collected_data:
        collected_data["chart_setup"] = {"title": f"ì§€ì ë³„ ë§¤ì¶œ ì¶”ì´ ë¹„êµ ({', '.join(store_codes)})"}
        total_sales = sum([float(r['total_sales']) for r in collected_data["daily_trend"] if r['total_sales']])
        total_orders = sum([int(r['total_orders']) for r in collected_data["daily_trend"] if r['total_orders']])
        
        collected_data["key_metrics"] = {
            "period": "ìµœê·¼ 7ì¼",
            "total_sales": total_sales,
            "total_orders": total_orders
        }

    # ê°„ë‹¨ ì§„ë‹¨ ì½”ë©˜íŠ¸ (íƒ€ì´í‹€ìš©)
    collected_data["diagnosis_result"] = f"ë¶„ì„ ì™„ë£Œ: {', '.join(store_codes)} (ìµœê·¼ 7ì¼)"

    state["sales_data"] = collected_data
    return state



==================================================
FILE_PATH: app\inquiry\nodes\save.py
==================================================

from app.inquiry.inquiry_schema import InquiryState
from app.inquiry.inquiry_service import save_inquiry

# ===== Step 7: Save Node (DB ì €ì¥) =====
async def save_node(state: InquiryState) -> InquiryState:
    """ì§ˆë¬¸ê³¼ ë‹µë³€ì„ DBì— ì €ì¥"""
    inquiry_id = save_inquiry(
        store_id=state["store_id"],
        category=state["category"],
        question=state["question"],
        answer=state["final_answer"]
    )
    
    state["inquiry_id"] = inquiry_id
    print(f"ğŸ’¾ [Save] DB ì €ì¥ ì™„ë£Œ (ID: {inquiry_id})")
    
    return state



==================================================
FILE_PATH: app\manual\manual_router.py
==================================================

from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from typing import List
from app.core.db import get_db
from app.manual.manual_schema import Manual, ManualResponse

router = APIRouter(prefix="/manual", tags=["Manual"])

@router.get("/get", response_model=List[ManualResponse])
def get_manuals(db: Session = Depends(get_db)):
    """ëª¨ë“  ë§¤ë‰´ì–¼ ì¡°íšŒ"""
    results = db.query(Manual).all()
    # Pydantic ëª¨ë¸(ManualResponse)ë¡œ ìë™ ë³€í™˜ë˜ì–´ ë¦¬í„´ë¨ (Vector ë“± ì œì™¸ë¨)
    return results



==================================================
FILE_PATH: app\manual\manual_schema.py
==================================================

from datetime import datetime
from sqlalchemy import Column, Integer, String, Text, DateTime
from sqlalchemy.sql import func
from pgvector.sqlalchemy import Vector
from pydantic import BaseModel
from typing import Optional, List
from app.core.db import base

class Manual(base):
    __tablename__ = "manuals"

    manual_id = Column(Integer, primary_key=True, index=True)
    category = Column(String(50), nullable=False)   # ì˜ˆ: ê¸°ê¸° ê´€ë¦¬, ìœ„ìƒ ê´€ë¦¬, í¬ìŠ¤ ìš´ì˜
    title = Column(String(200), nullable=False)
    content = Column(Text, nullable=False)
    
    # RAG ê²€ìƒ‰ìš© ì„ë² ë”©
    embedding = Column(Vector(1536), nullable=True)
    created_at = Column(DateTime(timezone=True), server_default=func.now())



# --- Pydantic Models for API ---
class ManualCreate(BaseModel):
    category: str
    title: str
    content: str



class ManualResponse(ManualCreate):
    manual_id: int
    created_at: datetime
    
    class Config:
        from_attributes = True



==================================================
FILE_PATH: app\menu\menu_router.py
==================================================

from fastapi import APIRouter
from app.menu.menu_service import select_menus_all

# router = APIRouter(prefix="/menu", tags=["menu"])
router = APIRouter()

@router.get("/menu/get")
async def get_menus_all():
    results = await select_menus_all()
    return results



==================================================
FILE_PATH: app\menu\menu_schema.py
==================================================

from pydantic import BaseModel
from sqlalchemy import Column, Integer, String, Boolean, Numeric, Text
from pgvector.sqlalchemy import Vector
from app.core.db import base

# ---------- API / JSON ìš© Pydantic ìŠ¤í‚¤ë§ˆ ----------

class MenuSchema(BaseModel):
    menu_id: int | None = None
    menu_name: str
    category: str
    base_price: float | None = None
    cost_price: float | None = None
    list_price: float | None = None
    ingredients: str | None = None
    recipe_steps: str | None = None
    is_seasonal: bool = False

# ---------- Alembic / DB ë§¤í•‘ìš© SQLAlchemy ëª¨ë¸ ----------

class Menu(base):
    __tablename__ = "menus"

    menu_id = Column(Integer, primary_key=True, index=True)
    menu_name = Column(String(100), nullable=False, index=True)
    category = Column(String(50), nullable=False)   # coffee / dessert
    is_seasonal = Column(Boolean, default=False)

    cost_price = Column(Numeric(10, 2), nullable=True)       # ì›ê°€
    list_price = Column(Numeric(10, 2), nullable=True)       # ì •ê°€
    
    # ë ˆì‹œí”¼ ì •ë³´ (Merged from Recipe table)
    ingredients = Column(Text, nullable=True)                # ìƒì„¸ ì¬ë£Œ: ì—ìŠ¤í”„ë ˆì†Œ 2ìƒ·, ìš°ìœ  200ml...
    recipe_steps = Column(Text, nullable=True)               # ì œì¡° ìˆœì„œ: 1. ìƒ· ì¶”ì¶œ 2. ìš°ìœ  ìŠ¤íŒ€...

    description = Column(String(500), nullable=True)         # ë©”ë‰´ ì„¤ëª… (ì„ë² ë”© ëŒ€ìƒ)
    embedding = Column(Vector(1536), nullable=True)   # ë©”ë‰´ ì¶”ì²œ/ê²€ìƒ‰ìš© ì„ë² ë”©



==================================================
FILE_PATH: app\menu\menu_service.py
==================================================

from app.core.db import fetch_all


async def select_menus_all():
    sql = "SELECT * FROM menus"
    rows = await fetch_all(sql)
    return rows



==================================================
FILE_PATH: app\order\order_router.py
==================================================

from fastapi import APIRouter
from app.order.order_service import select_orders_by_store, select_daily_sales_by_store

router = APIRouter(prefix="/order", tags=["order"])


@router.get("/store/{store_id}")
async def get_orders_by_store(store_id: int):
    return await select_orders_by_store(store_id)


@router.get("/store/{store_id}/daily_sales")
async def get_daily_sales_by_store(store_id: int):
    return await select_daily_sales_by_store(store_id)



==================================================
FILE_PATH: app\order\order_schema.py
==================================================

from pydantic import BaseModel
from datetime import datetime
from sqlalchemy import Column, Integer, Numeric, DateTime, ForeignKey
from app.core.db import base

# ---------- API / JSON ìš© Pydantic ìŠ¤í‚¤ë§ˆ ----------


class OrderSchema(BaseModel):
    order_id: int
    store_id: int
    menu_id: int
    quantity: int
    total_price: float
    ordered_at: datetime

# ---------- Alembic / DB ë§¤í•‘ìš© SQLAlchemy ëª¨ë¸ ----------


class Order(base):
    __tablename__ = "orders"

    order_id = Column(Integer, primary_key=True, index=True)
    store_id = Column(Integer, ForeignKey("stores.store_id"), nullable=False)
    menu_id = Column(Integer, ForeignKey("menus.menu_id"), nullable=False)
    quantity = Column(Integer, nullable=False)
    total_price = Column(Numeric(10, 2), nullable=False)
    ordered_at = Column(DateTime, default=datetime.now, nullable=False)



==================================================
FILE_PATH: app\order\order_service.py
==================================================

from app.core.db import fetch_all


async def select_orders_by_store(store_id: int):
    sql = """
        SELECT o.*, m.menu_name, m.category
        FROM orders o
        JOIN menus m ON o.menu_id = m.menu_id
        WHERE o.store_id = %s
        ORDER BY o.ordered_at DESC
    """
    rows = await fetch_all(sql, (store_id,))
    return rows


async def select_daily_sales_by_store(store_id: int):
    sql = """
        SELECT sale_date as order_date, total_sales as daily_revenue, total_orders as order_count, COALESCE(weather_info, 'ì•Œìˆ˜ì—†ìŒ') as weather_info
        FROM sales_daily
        WHERE store_id = %s
        ORDER BY sale_date ASC
    """
    rows = await fetch_all(sql, (store_id,))
    return rows


async def select_menu_sales_comparison(store_id: int, days: int = 7, target_date: str = None):
    """
    ìµœê·¼ Nì¼ vs ì´ì „ Nì¼ ë©”ë‰´ë³„ íŒë§¤ëŸ‰/ë§¤ì¶œ ë¹„êµ
    Args:
        store_id (int): ë§¤ì¥ ID
        days (int): ë¹„êµí•  ê¸°ê°„ (ê¸°ë³¸ 7ì¼)
        target_date (str): ê¸°ì¤€ ë‚ ì§œ (YYYY-MM-DD), ì—†ìœ¼ë©´ CURRENT_DATE ì‚¬ìš©
    """
    # ê¸°ì¤€ ë‚ ì§œ ì²˜ë¦¬ (SQL Injection ë°©ì§€ë¥¼ ìœ„í•´ íŒŒë¼ë¯¸í„° ë°”ì¸ë”© ì‚¬ìš©)
    base_date_expr = "CAST(%s AS DATE)" if target_date else "CURRENT_DATE"

    sql = f"""
        SELECT 
            m.menu_name, 
            m.category,
            COALESCE(SUM(CASE WHEN o.ordered_at >= {base_date_expr} - make_interval(days => %s) AND o.ordered_at < {base_date_expr} + interval '1 day' THEN o.total_price ELSE 0 END), 0) as recent_revenue,
            COUNT(CASE WHEN o.ordered_at >= {base_date_expr} - make_interval(days => %s) AND o.ordered_at < {base_date_expr} + interval '1 day' THEN 1 ELSE NULL END) as recent_count,
            COALESCE(SUM(CASE WHEN o.ordered_at < {base_date_expr} - make_interval(days => %s) AND o.ordered_at >= {base_date_expr} - make_interval(days => %s) THEN o.total_price ELSE 0 END), 0) as prev_revenue,
            COUNT(CASE WHEN o.ordered_at < {base_date_expr} - make_interval(days => %s) AND o.ordered_at >= {base_date_expr} - make_interval(days => %s) THEN 1 ELSE NULL END) as prev_count
        FROM orders o
        JOIN menus m ON o.menu_id = m.menu_id
        WHERE o.store_id = %s
        AND o.ordered_at >= {base_date_expr} - make_interval(days => %s)
        AND o.ordered_at < {base_date_expr} + interval '1 day'
        GROUP BY m.menu_name, m.category
        ORDER BY recent_revenue DESC
    """
    
    # íŒŒë¼ë¯¸í„° ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    # target_dateê°€ ìˆìœ¼ë©´ SQL ì•ë¶€ë¶„ì— ë°”ì¸ë”© í•„ìš”
    # ìˆœì„œ: (target_date...) -> days -> (target_date...) ...
    # ë³µì¡í•˜ë¯€ë¡œ, target_dateë¥¼ paramsì— në²ˆ ë„£ì–´ì£¼ëŠ” ë°©ì‹ë³´ë‹¤ëŠ”
    # f-stringìœ¼ë¡œ base_date_exprë¥¼ ë„£ì—ˆì§€ë§Œ, ì•ˆì „í•˜ê²Œ íŒŒë¼ë¯¸í„°ë¡œ ì²˜ë¦¬í•˜ë ¤ë©´ ë¡œì§ì´ ë³µì¡í•´ì§.
    # ì—¬ê¸°ì„œëŠ” target_dateê°€ ê²€ì¦ëœ YYYY-MM-DD ë¬¸ìì—´ì´ë¼ê³  ê°€ì •í•˜ê³ ,
    # ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ìˆœì„œë¥¼ ë§ì¶”ëŠ” ì •ê³µë²• ì‚¬ìš©.
    
    # Parameter order depends on how many times base_date_expr appears with %s.
    # If target_date is None, base_date_expr is "CURRENT_DATE" (no param).
    # If target_date is set, base_date_expr is "CAST(%s AS DATE)" (1 param).
    
    # Let's simplify: Use a CTE or variable in logic? No, simple param construction.
    
    p_date = [target_date] if target_date else []
    
    # Params corresponding to the SQL placeholders (%s)
    # 1. recent_rev start: days
    # 2. recent_cnt start: days
    # 3. prev_rev end: days, start: days*2
    # 4. prev_cnt end: days, start: days*2
    # 5. WHERE store_id
    # 6. WHERE start: days*2
    
    # Wait, because of base_date_expr expansion, we need to inject params properly.
    # If target_date:
    # SELECT ... CASE WHEN ... >= CAST(%s AS DATE) - ...
    # This interleaving is tricky.
    
    # Alternative: Use a WITH clause to define the anchor date once.
    
    sql = """
        WITH anchor AS (
            SELECT CAST(%s AS DATE) as ref_date
        )
        SELECT 
            m.menu_name, 
            m.category,
            COALESCE(SUM(CASE WHEN o.ordered_at >= (SELECT ref_date FROM anchor) - make_interval(days => %s) 
                              AND o.ordered_at < (SELECT ref_date FROM anchor) + interval '1 day' THEN o.total_price ELSE 0 END), 0) as recent_revenue,
            COUNT(CASE WHEN o.ordered_at >= (SELECT ref_date FROM anchor) - make_interval(days => %s) 
                              AND o.ordered_at < (SELECT ref_date FROM anchor) + interval '1 day' THEN 1 ELSE NULL END) as recent_count,
            COALESCE(SUM(CASE WHEN o.ordered_at < (SELECT ref_date FROM anchor) - make_interval(days => %s) 
                              AND o.ordered_at >= (SELECT ref_date FROM anchor) - make_interval(days => %s) THEN o.total_price ELSE 0 END), 0) as prev_revenue,
            COUNT(CASE WHEN o.ordered_at < (SELECT ref_date FROM anchor) - make_interval(days => %s) 
                              AND o.ordered_at >= (SELECT ref_date FROM anchor) - make_interval(days => %s) THEN 1 ELSE NULL END) as prev_count
        FROM orders o
        JOIN menus m ON o.menu_id = m.menu_id
        WHERE o.store_id = %s
        AND o.ordered_at >= (SELECT ref_date FROM anchor) - make_interval(days => %s)
        AND o.ordered_at < (SELECT ref_date FROM anchor) + interval '1 day'
        GROUP BY m.menu_name, m.category
        ORDER BY recent_revenue DESC
    """
    
    # If target_date is None, use current date string
    if not target_date:
        from datetime import date
        target_date = str(date.today())
        
    # Query structure analysis:
    # 1. WITH anchor: %s (target_date)
    # 2. recent_rev: %s (days)
    # 3. recent_cnt: %s (days)
    # 4. prev_rev: %s (days), %s (days*2)
    # 5. prev_cnt: %s (days), %s (days*2)
    # 6. WHERE store_id: %s
    # 7. WHERE start: %s (days*2)
    
    params = (target_date, days, days, days, days * 2, days, days * 2, store_id, days * 2)
    
    rows = await fetch_all(sql, params)
    return rows


async def select_sales_by_day_type(store_id: int, days: int = 7, target_date: str = None):
    """
    ìµœê·¼ Nì¼ vs ì´ì „ Nì¼ì˜ 'í‰ì¼(Weekday)' vs 'ì£¼ë§(Weekend)' ë§¤ì¶œ ë¹„êµ
    """
    if not target_date:
        from datetime import date
        target_date = str(date.today())

    sql = """
        WITH anchor AS (
            SELECT CAST(%s AS DATE) as ref_date
        )
        SELECT 
            CASE WHEN EXTRACT(ISODOW FROM ordered_at) IN (6, 7) THEN 'Weekend' ELSE 'Weekday' END as day_type,
            
            SUM(CASE WHEN ordered_at >= (SELECT ref_date FROM anchor) - make_interval(days => %s) 
                     AND ordered_at < (SELECT ref_date FROM anchor) + interval '1 day' THEN total_price ELSE 0 END) as recent_revenue,
            COUNT(CASE WHEN ordered_at >= (SELECT ref_date FROM anchor) - make_interval(days => %s) 
                     AND ordered_at < (SELECT ref_date FROM anchor) + interval '1 day' THEN 1 ELSE NULL END) as recent_count,
            
            SUM(CASE WHEN ordered_at < (SELECT ref_date FROM anchor) - make_interval(days => %s) 
                     AND ordered_at >= (SELECT ref_date FROM anchor) - make_interval(days => %s) THEN total_price ELSE 0 END) as prev_revenue,
            COUNT(CASE WHEN ordered_at < (SELECT ref_date FROM anchor) - make_interval(days => %s) 
                     AND ordered_at >= (SELECT ref_date FROM anchor) - make_interval(days => %s) THEN 1 ELSE NULL END) as prev_count
        
        FROM orders
        WHERE store_id = %s
        AND ordered_at >= (SELECT ref_date FROM anchor) - make_interval(days => %s)
        AND ordered_at < (SELECT ref_date FROM anchor) + interval '1 day'
        GROUP BY 1
    """
    
    # Query structure analysis:
    # 1. WITH anchor: %s (target_date)
    # 2. recent_rev: %s (days)
    # 3. recent_cnt: %s (days)
    # 4. prev_rev: %s (days), %s (days*2)
    # 5. prev_cnt: %s (days), %s (days*2)
    # 6. WHERE store_id: %s
    # 7. WHERE start: %s (days*2)
    
    params = (target_date, days, days, days, days * 2, days, days * 2, store_id, days * 2)
    rows = await fetch_all(sql, params)
    return rows



==================================================
FILE_PATH: app\policy\policy_router.py
==================================================

from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session
from typing import List
from app.core.db import get_db
from app.policy.policy_schema import Policy, PolicyResponse

router = APIRouter(prefix="/policy", tags=["Policy"])

@router.get("/get", response_model=List[PolicyResponse])
def get_policies(db: Session = Depends(get_db)):
    """ëª¨ë“  ì •ì±…/ê·œì • ì¡°íšŒ"""
    results = db.query(Policy).all()
    # Pydantic ëª¨ë¸(PolicyResponse)ë¡œ ìë™ ë³€í™˜ (ì„ë² ë”© í•„ë“œ ì œì™¸)
    return results



==================================================
FILE_PATH: app\policy\policy_schema.py
==================================================

from datetime import datetime
from sqlalchemy import Column, Integer, String, Text, DateTime
from sqlalchemy.sql import func
from pgvector.sqlalchemy import Vector
from pydantic import BaseModel
from typing import Optional, List
from app.core.db import base

# ---------- Alembic / DB ë§¤í•‘ìš© SQLAlchemy ëª¨ë¸ ----------
class Policy(base):
    __tablename__ = "policies"

    policy_id = Column(Integer, primary_key=True, index=True)
    category = Column(String(50), nullable=False)   # ì˜ˆ: ì¸ì‚¬, ë³´ì•ˆ, ì•ˆì „, ê³„ì•½
    title = Column(String(200), nullable=False)     # ì •ì±… ì œëª©
    content = Column(Text, nullable=False)          # ìƒì„¸ ì •ì±… ë‚´ìš©
    
    # RAG ê²€ìƒ‰ì„ ìœ„í•œ ë²¡í„° ë°ì´í„° (OpenAI Embedding: 1536 dim)
    embedding = Column(Vector(1536), nullable=True)
    
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())

# ---------- API / JSON ìš© Pydantic ìŠ¤í‚¤ë§ˆ ----------
class PolicyCreate(BaseModel):
    category: str
    title: str
    content: str

class PolicyResponse(BaseModel):
    policy_id: int
    category: str
    title: str
    content: str
    created_at: datetime
    
    class Config:
        from_attributes = True



==================================================
FILE_PATH: app\report\report_graph.py
==================================================

import json
from typing import Annotated, TypedDict, List, Dict, Any
from datetime import date
from langgraph.graph import StateGraph, END
from app.core.db import SessionLocal
from app.report.report_schema import StoreReport
from app.order.order_service import select_daily_sales_by_store, select_menu_sales_comparison, select_sales_by_day_type
from app.review.review_service import select_reviews_by_store
from app.clients.genai import genai_generate_text
from app.clients.weather import fetch_weather_data

from app.core.db import fetch_all
from datetime import datetime, timedelta

from langgraph.graph.message import add_messages

# ë¦¬ìŠ¤íŠ¸ë¥¼ ë®ì–´ì“°ì§€ ì•Šê³  ì¶”ê°€í•˜ê¸° ìœ„í•œ ë¦¬ë“€ì„œ í•¨ìˆ˜
def append_logs(left: List[str], right: List[str]) -> List[str]:
    return left + right

# 1. ê·¸ë˜í”„ ìƒíƒœ(State) ì •ì˜
class ReportState(TypedDict):
    store_id: int
    store_name: str
    target_date: str # [Optional] ë¶„ì„ ê¸°ì¤€ ë‚ ì§œ (YYYY-MM-DD)
    sales_data: List[Dict[str, Any]]      # ì´ë²ˆì£¼ ë§¤ì¶œ (ìµœê·¼ 7ì¼)
    prev_sales_data: List[Dict[str, Any]] # ì§€ë‚œì£¼ ë§¤ì¶œ (ê·¸ ì „ 7ì¼)
    reviews_data: List[Dict[str, Any]]
    menu_sales_data: List[Dict[str, Any]]
    weather_data: Dict[str, str]
    # [NEW] ì§‘ê³„ ì •í•©ì„±ì„ ìœ„í•´ fetch ë‹¨ê³„ì—ì„œ ê³„ì‚°í•œ ê°’ì„ ë„˜ê¹€
    calculated_total_sales: float 
    calculated_prev_sales: float
    final_report: Dict[str, Any]
    execution_logs: Annotated[List[str], append_logs]

async def fetch_data_node(state: ReportState):
    """DBì—ì„œ ë§¤ì¶œê³¼ ë¦¬ë·° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë…¸ë“œ"""
    store_id = state["store_id"]
    log = f"ğŸ“Š [Fetch] '{state['store_name']}' ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"
    print(log)

    # 1. ê¸°ì¤€ ë‚ ì§œ(Anchor Date) ê²°ì •
    # ì‹œì—° ëª¨ë“œ or ê³¼ê±° ë‚ ì§œ ì¡°íšŒ ì§€ì›
    target_date_str = state.get("target_date")
    

    if not target_date_str:
        # íƒ€ê²Ÿ ë‚ ì§œê°€ ì—†ìœ¼ë©´ DB ìµœì‹  ë‚ ì§œ ì¡°íšŒ (Simulation Mode)
        try:
            max_date_rows = await fetch_all(f"""
                SELECT MAX(sale_date) as last_date 
                FROM sales_daily 
                WHERE store_id = {store_id}
            """)
            if max_date_rows and max_date_rows[0]['last_date']:
                target_date_str = str(max_date_rows[0]['last_date'])
                log += f"\nğŸ•’ ìµœì‹  ë°ì´í„° ë‚ ì§œ ê¸°ì¤€: {target_date_str}"
            else:
                target_date_str = str(date.today())
        except:
            target_date_str = str(date.today())


    ref_date = datetime.strptime(target_date_str, "%Y-%m-%d").date()
    
    # 2. ë°ì´í„° ì¡°íšŒ
    # ë©”ë‰´ë³„, ìš”ì¼ë³„ í†µê³„ëŠ” ê¸°ì¤€ ë‚ ì§œë¥¼ ë„˜ê²¨ì„œ DBì—ì„œ ì •í™•íˆ ê³„ì‚°
    menu_stats = await select_menu_sales_comparison(store_id, days=7, target_date=target_date_str)
    # day_stats = await select_sales_by_day_type(store_id, days=7, target_date=target_date_str) # [ì‚­ì œ] DB í˜¸ì¶œ ëŒ€ì‹  ì§ì ‘ ì§‘ê³„
    reviews = await select_reviews_by_store(store_id) # ë¦¬ë·°ëŠ” ì „ì²´ ê°€ì ¸ì™€ì„œ ìµœì‹ ìˆœ (TODO: ë‚ ì§œ í•„í„°ë§?)

    # ì¼ë³„ ë§¤ì¶œì€ ì „ì²´ë¥¼ ê°€ì ¸ì˜¨ ë’¤ íŒŒì´ì¬ì—ì„œ ë‚ ì§œ í•„í„°ë§ (DB í˜¸ì¶œ íšŸìˆ˜ ì ˆì•½)
    all_sales = await select_daily_sales_by_store(store_id)
    
    # 3. ë‚ ì§œ í•„í„°ë§ (ì´ë²ˆì£¼ vs ì§€ë‚œì£¼)
    # ì´ë²ˆì£¼: ref_date í¬í•¨ ìµœê·¼ 7ì¼ (ref_date - 6 ~ ref_date)
    # ì§€ë‚œì£¼: ê·¸ ì „ 7ì¼ (ref_date - 13 ~ ref_date - 7)
    
    curr_start = ref_date - timedelta(days=6)
    curr_end = ref_date
    prev_start = ref_date - timedelta(days=13)
    prev_end = ref_date - timedelta(days=7)
    
    target_sales = []
    prev_sales = []

    # [NEW] íŒŒì´ì¬ ë‚´ë³´ë‚´ê¸° ì§‘ê³„ (í‰ì¼/ì£¼ë§ ì •í•©ì„± ë³´ì¥)
    weekday_sales = {"recent": 0, "prev": 0}
    weekend_sales = {"recent": 0, "prev": 0}
    
    for s in all_sales:
        s_date = s['order_date'] # date object
        rev = float(s['daily_revenue'])

        # ì´ë²ˆì£¼ ë°ì´í„°
        if curr_start <= s_date <= curr_end:
            target_sales.append(s)
            if s_date.weekday() < 5: # 0~4: í‰ì¼
                weekday_sales["recent"] += rev
            else: # 5~6: ì£¼ë§
                weekend_sales["recent"] += rev

        # ì§€ë‚œì£¼ ë°ì´í„°
        elif prev_start <= s_date <= prev_end:
            prev_sales.append(s)
            if s_date.weekday() < 5:
                weekday_sales["prev"] += rev
            else:
                weekend_sales["prev"] += rev
            
    # ì •ë ¬ (ë‚ ì§œ ì˜¤ë¦„ì°¨ìˆœ) -> ê·¸ë˜í”„ìš©
    target_sales.sort(key=lambda x: x['order_date'])
    prev_sales.sort(key=lambda x: x['order_date'])

    # weather_map êµ¬ì„±
    weather_map = {str(s['order_date']): s.get('weather_info', 'ì•Œìˆ˜ì—†ìŒ') for s in target_sales}

    return {
        "sales_data": target_sales,
        "prev_sales_data": prev_sales,
        "reviews_data": reviews[:15], # ìµœì‹  15ê°œë§Œ ì‚¬ìš©
        "menu_sales_data": menu_stats,
        "weather_data": weather_map,
        "calculated_total_sales": weekday_sales["recent"] + weekend_sales["recent"], # [NEW] ì •í™•í•œ í•©ê³„ ì „ë‹¬
        "calculated_prev_sales": weekday_sales["prev"] + weekend_sales["prev"],
        "target_date": target_date_str, # State ì—…ë°ì´íŠ¸
        "execution_logs": [log, f"âœ… [Fetch] ë°ì´í„° ìˆ˜ì§‘ ë° ì •í•©ì„± ê²€ì¦ ì™„ë£Œ (ê¸°ì¤€ì¼: {target_date_str})"]
    }

async def analyze_data_node(state: ReportState):
    """ë°ì´í„° ë¶„ì„ ë° ìˆ˜ì¹˜ì  ê·¼ê±° ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ë…¸ë“œ"""
    log = "ğŸ§  [Analyze] ìˆ˜ì¹˜ ë°ì´í„° ê³„ì‚° ë° AI ë¶„ì„ ì‹œì‘"
    print(log)

    sales = state["sales_data"]     # ì´ë²ˆì£¼
    prev_sales = state.get("prev_sales_data", []) # ì§€ë‚œì£¼
    reviews = state["reviews_data"]
    menu_stats = state.get("menu_sales_data", [])
    
    # 1. ì£¼ê°„ ë§¤ì¶œ ë¹„êµ (Weekly Comparison)
    # [ë³€ê²½] fetch ë‹¨ê³„ì—ì„œ ê³„ì‚°ëœ ì •í™•í•œ í•©ê³„ ì‚¬ìš© (ì¬ê³„ì‚° X)
    this_week_total = state["calculated_total_sales"]
    prev_week_total = state.get("calculated_prev_sales", 0)
    
    avg_rev = this_week_total / len(sales) if sales else 0
    
    # ì„±ì¥ë¥  ê³„ì‚° (Growth Rate)
    if prev_week_total > 0:
        growth_rate = ((this_week_total - prev_week_total) / prev_week_total * 100)
    else:
        growth_rate = 100 if this_week_total > 0 else 0

    avg_rating = sum(r['rating'] for r in reviews) / len(reviews) if reviews else 0

    # ë©”ë‰´ë³„ ì¦ê° ë¶„ì„ (ë§¤ì¶œ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ëœ ìƒíƒœ)
    processed_menus = []
    for m in menu_stats:
        rec_rev = float(m['recent_revenue'])
        prev_rev = float(m['prev_revenue'])
        change_pct = ((rec_rev - prev_rev) / prev_rev * 100) if prev_rev > 0 else (100 if rec_rev > 0 else 0)
        
        item = {
            "menu": m['menu_name'],
            "cat": m['category'],
            "recent_rev": rec_rev,
            "prev_rev": prev_rev,
            "change_pct": round(change_pct, 1)
        }
        processed_menus.append(item)
    
    # 1. Top Selling (ë§¤ì¶œì•¡ ìƒìœ„)
    top_selling = sorted(processed_menus, key=lambda x: x['recent_rev'], reverse=True)[:5]

    # 2. Top Dropping (ê°ì†Œí­ í•˜ìœ„) - ì—­ì„±ì¥ ë©”ë‰´
    dropping_candidates = [m for m in processed_menus if m['prev_rev'] > 0]
    worst_dropping = sorted(dropping_candidates, key=lambda x: x['change_pct'])[:5]

    # UIìš© ì›ë³¸ ë°ì´í„° ìš”ì•½ (ë‚ ì§œ, ë§¤ì¶œë§Œ) + ë‚ ì”¨ ì¶”ê°€
    source_sales = []
    for s in sales:
        d_str = str(s['order_date'])
        source_sales.append({
            "date": d_str,
            "revenue": float(s['daily_revenue']),
            "weather": s.get('weather_info', "ì•Œìˆ˜ì—†ìŒ")
        })

    prompt = f"""
    í”„ëœì°¨ì´ì¦ˆ ê²½ì˜ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  **ìˆ˜ì¹˜ì  ê·¼ê±°**ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ê²°ì±…ì„ ì œì‹œí•´ì¤˜.
    ë§¤ì¥: {state['store_name']}
    
    [ì£¼ê°„ ë§¤ì¶œ ìš”ì•½]
    - ì´ë²ˆì£¼ ì´ ë§¤ì¶œ(7ì¼): {int(this_week_total):,}ì›
    - ì§€ë‚œì£¼ ì´ ë§¤ì¶œ(7ì¼): {int(prev_week_total):,}ì›
    - ì£¼ê°„ ì„±ì¥ë¥ (WoW): {growth_rate:+.1f}%
    - ì´ë²ˆì£¼ í‰ê·  ë³„ì : {avg_rating:.1f}ì 
    
    ìƒì„¸ ë§¤ì¶œ ë‚´ì—­ (ë‚ ì”¨ í¬í•¨): {json.dumps(source_sales, ensure_ascii=False)}
    ìƒì„¸ ë¦¬ë·° ë‚´ì—­: {json.dumps([{"rate": r['rating'], "txt": r['review_text']} for r in reviews], ensure_ascii=False)}
    
    [ë©”ë‰´ ë¶„ì„]
    ì˜ íŒ”ë¦° ë©”ë‰´ (TOP 5): {json.dumps(top_selling, ensure_ascii=False)}
    ê¸‰ê°í•œ ë©”ë‰´ (WORST 5): {json.dumps(worst_dropping, ensure_ascii=False)}

    ë¶„ì„ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ë°˜ë“œì‹œ ì§€ì¼œì¤˜:
    1. **"ì´ë²ˆì£¼ ë§¤ì¶œì´ ì§€ë‚œì£¼ ëŒ€ë¹„ ì™œ ë³€í–ˆëŠ”ê°€?"**ë¥¼ í•µì‹¬ ì£¼ì œë¡œ ì¡ìœ¼ì„¸ìš”. (ì„±ì¥ ë˜ëŠ” í•˜ë½ì˜ ì›ì¸ ê·œëª…)
    2. **ë‚ ì”¨ì™€ ë§¤ì¶œì˜ ìƒê´€ê´€ê³„**ë¥¼ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ì„¸ìš”. 
       - "ì§€ë‚œì£¼ ëŒ€ë¹„ ë¹„ì˜¤ëŠ” ë‚ ì´ ë§ì•„ ë°°ë‹¬ ë§¤ì¶œì´ ëŠ˜ì—ˆë‹¤" ë“± êµ¬ì²´ì ìœ¼ë¡œ.
    3. ìˆ˜ì¹˜ì  ê·¼ê±°(Top 5 ë©”ë‰´ëª…, ì£¼ë§ ë§¤ì¶œ ë³€ë™ë¥  ë“±)ë¥¼ í¬í•¨í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ì‹œê°í™”í•˜ì„¸ìš”.
    
    ì‘ë‹µì€ ë°˜ë“œì‹œ ì•„ë˜ íƒœê·¸ í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ì‘ì„±í•  ê²ƒ (JSON ì•„ë‹˜):

    <SECTION:SALES_ANALYSIS>
    ì£¼ê°„ ë§¤ì¶œ ë¹„êµ, ë‚ ì”¨, ë©”ë‰´ ë°ì´í„°ë¥¼ ì¢…í•©í•œ ìƒì„¸ ë¶„ì„ ë‚´ìš© (ë§ˆí¬ë‹¤ìš´ í‘œ í¬í•¨)
    </SECTION:SALES_ANALYSIS>

    <SECTION:SUMMARY>
    í•µì‹¬ ìš”ì•½ (ì§€ë‚œì£¼ ëŒ€ë¹„ ë³€ë™ ì›ì¸ í¬í•¨ 3ì¤„)
    </SECTION:SUMMARY>

    <SECTION:STRATEGY>
    ë‹¤ìŒì£¼ ë§¤ì¶œ ì¦ëŒ€ë¥¼ ìœ„í•œ ë‚ ì”¨/íŠ¸ë Œë“œ ê¸°ë°˜ ë§ˆì¼€íŒ… ì œì•ˆ
    </SECTION:STRATEGY>

    <SECTION:IMPROVEMENT>
    ë§¤ì¥ ìš´ì˜ íš¨ìœ¨í™” ë° ì„œë¹„ìŠ¤ ê°œì„  ì œì•ˆ
    </SECTION:IMPROVEMENT>

    <SECTION:RISK>
    {{"risk_score": 80, "main_risks": ["ë¦¬ìŠ¤í¬ì˜ˆì‹œ1", "ë¦¬ìŠ¤í¬ì˜ˆì‹œ2"], "suggestion": "ê°œì„ ë°©ì•ˆ"}}
    </SECTION:RISK>
    """

    raw_text = await genai_generate_text(prompt)
    
    # [NEW] Tag-based Parsing Logic (Robust)
    import re
    
    def extract_section(tag, text):
        pattern = f"<{tag}>(.*?)</{tag}>"
        match = re.search(pattern, text, re.DOTALL)
        return match.group(1).strip() if match else ""

    try:
        sales_analysis = extract_section("SECTION:SALES_ANALYSIS", raw_text)
        summary = extract_section("SECTION:SUMMARY", raw_text)
        strategy = extract_section("SECTION:STRATEGY", raw_text)
        improvement = extract_section("SECTION:IMPROVEMENT", raw_text)
        risk_text = extract_section("SECTION:RISK", raw_text)
        
        # Risk ì„¹ì…˜ë§Œ JSON íŒŒì‹± ì‹œë„ (êµ¬ì¡°í™”ëœ ë°ì´í„°ê°€ í•„ìš”í•˜ë¯€ë¡œ)
        risk_data = {"risk_score": 0, "main_risks": [], "suggestion": ""}
        if risk_text:
            try:
                # ì œì–´ ë¬¸ì ì œê±° í›„ íŒŒì‹±
                risk_text_clean = re.sub(r'[\x00-\x1F\x7F]', '', risk_text)
                risk_data = json.loads(risk_text_clean, strict=False)
            except:
                print(f"âš ï¸ Risk JSON íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©. Valid Text: {risk_text[:100]}")

        # í•„ìˆ˜ ì„¹ì…˜ì´ ë¹„ì–´ìˆìœ¼ë©´ ì‹¤íŒ¨ë¡œ ê°„ì£¼ (ìµœì†Œ sales_analysisëŠ” ìˆì–´ì•¼ í•¨)
        if not sales_analysis:
             raise ValueError("Main analysis section missing")

        report_dict = {
            "data_evidence": {"sales_analysis": sales_analysis},
            "summary": summary,
            "marketing_strategy": strategy,
            "operational_improvement": improvement,
            "risk_assessment": risk_data
        }

    except Exception as e:
        print(f"âŒ [Analyze] íƒœê·¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
        print("--- [AI Raw Output Start] ---")
        print(raw_text)
        print("--- [AI Raw Output End] ---")
        
        # Fallback
        report_dict = {
            "data_evidence": {"sales_analysis": "ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨ (í˜•ì‹ ì˜¤ë¥˜)"},
            "summary": "ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "marketing_strategy": "",
            "operational_improvement": "",
            "risk_assessment": {"risk_score": 0, "main_risks": [], "suggestion": ""}
        }
    
    # UIìš© í†µê³„ ë°ì´í„° ë° ì†ŒìŠ¤ ë°ì´í„° ì¶”ê°€
    report_dict["metrics"] = {
        "total_rev": this_week_total,
        "avg_rev": avg_rev,
        "trend_percent": growth_rate, # íŠ¸ë Œë“œ ëŒ€ì‹  ì„±ì¥ë¥  ì‚¬ìš©
        "avg_rating": avg_rating,
        "prev_total_rev": prev_week_total # ì§€ë‚œì£¼ ë§¤ì¶œ ì¶”ê°€
    }
    report_dict["source_data"] = {
        "recent_sales": source_sales,
        "review_count": len(reviews),
        "top_selling_menus": top_selling,
        "worst_dropping_menus": worst_dropping,
    }

    return {
        "final_report": report_dict,
        "execution_logs": [log, f"âœ… [Analyze] ìˆ˜ì¹˜ ê·¼ê±° ë¶„ì„ ì™„ë£Œ (ì£¼ê°„ ì„±ì¥ë¥ : {growth_rate:+.1f}%)"]
    }

async def save_report_node(state: ReportState):
    """ìµœì¢… ë¦¬í¬íŠ¸ë¥¼ DBì— ì €ì¥í•˜ëŠ” ë…¸ë“œ"""
    log = "ğŸ’¾ [Save] ë¶„ì„ ê²°ê³¼ DB ì €ì¥ ì¤‘"
    report_dict = state["final_report"]

    # ë©”íŠ¸ë¦­ ë° ì†ŒìŠ¤ ì •ë³´ë¥¼ risk_assessment ë‚´ë¶€ì— ë³‘í•©í•˜ì—¬ ì˜êµ¬ ì €ì¥
    risk_info = report_dict.get('risk_assessment', {})
    risk_info['metrics'] = report_dict.get('metrics')
    risk_info['data_evidence'] = report_dict.get('data_evidence')
    risk_info['source_data'] = report_dict.get('source_data')  # ì›ë³¸ ë°ì´í„° ì¶”ê°€ ì €ì¥

    db_report = StoreReport(
        store_id=state["store_id"],
        report_date=date.today(),
        report_type="AI_GRAPH_REPORT",
        summary=report_dict['summary'],
        marketing_strategy=report_dict['marketing_strategy'],
        operational_improvement=report_dict['operational_improvement'],
        risk_assessment=risk_info
    )

    # Risk ì ìˆ˜ê°€ 0ì´ë©´ íŒŒì‹± ì‹¤íŒ¨ë¡œ ê°„ì£¼ -> DB ì €ì¥ ê±´ë„ˆë›°ê¸° (ì¬ì‹œë„ ìœ ë„)
    risk_score_val = risk_info.get('risk_score') if isinstance(risk_info, dict) else 0
    
    # [Prevent Saving Bad Data] 
    # íŒŒì‹± ì‹¤íŒ¨(0)ê±°ë‚˜ í•„ìˆ˜ í•„ë“œê°€ ì—†ìœ¼ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ.
    if not risk_score_val or risk_score_val == 0:
        return {
             "execution_logs": [log, "âš ï¸ [Skip Save] ë¶ˆì™„ì „í•œ ë¦¬í¬íŠ¸(Risk Parsing Fail)ë¡œ ì¸í•´ DB ì €ì¥ì„ ìƒëµí•©ë‹ˆë‹¤."]
        }

    with SessionLocal() as session:
        session.query(StoreReport).filter_by(
            store_id=state["store_id"],              
            report_date=date.today()).delete()
        session.add(db_report)
        session.commit()

    return {
        "execution_logs": [log, "ğŸ [Complete] í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ"]
    }

def create_report_graph():
    workflow = StateGraph(ReportState)
    workflow.add_node("fetch_data", fetch_data_node)
    workflow.add_node("analyze_data", analyze_data_node)
    workflow.add_node("save_report", save_report_node)

    workflow.set_entry_point("fetch_data")
    workflow.add_edge("fetch_data", "analyze_data")
    workflow.add_edge("analyze_data", "save_report")
    workflow.add_edge("save_report", END)

    return workflow.compile()

# [Singleton íŒ¨í„´] ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì»´íŒŒì¼í•˜ì—¬ ì¬ì‚¬ìš©
report_graph_app = create_report_graph()



==================================================
FILE_PATH: app\report\report_router.py
==================================================

from fastapi import APIRouter, HTTPException
from app.report.report_schema import GenerateReportRequest
from app.report.report_service import generate_ai_store_report, select_latest_report

router = APIRouter(prefix="/report", tags=["report"])


@router.post("/generate")
async def post_generate_report(request: GenerateReportRequest):
    """
    AI ì „ëµ ë¦¬í¬íŠ¸ ìƒì„± ìš”ì²­ (Request Body ì‚¬ìš© - store_id í¬í•¨)
    """
    result = await generate_ai_store_report(request.store_id, request.store_name, request.mode, request.target_date)
    if not result:
        raise HTTPException(status_code=500, detail="ë¦¬í¬íŠ¸ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    return result


@router.get("/latest/{store_id}")
async def get_latest_report(store_id: int):
    """
    í•´ë‹¹ ì§€ì ì˜ ê°€ì¥ ìµœê·¼ ë¦¬í¬íŠ¸ ì¡°íšŒ
    """
    report = await select_latest_report(store_id)
    if not report:
        return None
    return report

@router.delete("/reset/{store_id}")
async def delete_reports(store_id: int):
    """
    í•´ë‹¹ ì§€ì ì˜ ëª¨ë“  AI ë¦¬í¬íŠ¸ ë°ì´í„° ì‚­ì œ (ì´ˆê¸°í™”)
    """
    from app.core.db import SessionLocal
    from app.report.report_schema import StoreReport
    
    try:
        # 1. DB ì‚­ì œ
        with SessionLocal() as session:
            # í•´ë‹¹ ì§€ì  ë¦¬í¬íŠ¸ ì „ì²´ ì‚­ì œ
            session.query(StoreReport).filter(StoreReport.store_id == store_id).delete()
            session.commit()
            
        # 2. Redis ìºì‹œ ì‚­ì œ (ë™ê¸°í™”)
        from app.core.cache import get_redis
        client = await get_redis()
        if client:
            # í•´ë‹¹ store_idì˜ ëª¨ë“  ë¦¬í¬íŠ¸ í‚¤ ìŠ¤ìº”
            keys = await client.keys(f"report:{store_id}:*")
            if keys:
                await client.delete(*keys)
                print(f"ğŸ—‘ï¸ [Redis] {store_id}ë²ˆ ì§€ì  ê´€ë ¨ ìºì‹œ {len(keys)}ê°œ ì‚­ì œ ì™„ë£Œ")

        return {"status": "success", "message": f"{store_id}ë²ˆ ì§€ì  ë¦¬í¬íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))



==================================================
FILE_PATH: app\report\report_schema.py
==================================================

from pydantic import BaseModel
from datetime import date, datetime
from typing import Dict, Any, Optional
from sqlalchemy import Column, Integer, String, Text, Date, ForeignKey, JSON
from app.core.db import base

# ---------- API / JSON ìš© Pydantic ìŠ¤í‚¤ë§ˆ ----------

class GenerateReportRequest(BaseModel):
    store_id: int  # [NEW] Bodyì— í¬í•¨
    store_name: str
    mode: str = "sequential"
    target_date: Optional[str] = None # YYYY-MM-DD

class StoreReportSchema(BaseModel):
    report_id: int
    store_id: int
    report_date: date
    report_type: str  # DAILY, WEEKLY, MONTHLY

    summary: str
    marketing_strategy: str
    operational_improvement: str
    risk_assessment: Dict[str, Any] | None = None  # JSON ë°ì´í„°

# ---------- Alembic / DB ë§¤í•‘ìš© SQLAlchemy ëª¨ë¸ ----------


class StoreReport(base):
    __tablename__ = "store_reports"

    report_id = Column(Integer, primary_key=True, index=True)
    store_id = Column(Integer, ForeignKey("stores.store_id"), nullable=False)
    report_date = Column(Date, default=date.today, nullable=False)

    # ë¦¬í¬íŠ¸ ìœ í˜•: DAILY(ì¼ê°„), WEEKLY(ì£¼ê°„), MONTHLY(ì›”ê°„)
    report_type = Column(String(20), nullable=False)

    # ğŸ¤– AI ë¶„ì„ ê²°ê³¼ ì €ì¥ ì˜ì—­
    summary = Column(Text, nullable=True)                   # ì¢…í•© 3ì¤„ ìš”ì•½
    # ë§ˆì¼€íŒ… ì œì•ˆ (íƒ€ê²ŸíŒ…, í”„ë¡œëª¨ì…˜ ë“±)
    marketing_strategy = Column(Text, nullable=True)
    operational_improvement = Column(
        Text, nullable=True)   # ìš´ì˜ ê°œì„  ì œì•ˆ (ì¸ë ¥ ë°°ì¹˜, ì¬ê³  ê´€ë¦¬ ë“±)

    # êµ¬ì¡°í™”ëœ ë¶„ì„ ë°ì´í„° (JSON)
    # ì˜ˆ: {"risk_score": 85, "churn_prediction": "high", "top_keywords": ["ì¹œì ˆ", "ëŠë¦¼"]}
    risk_assessment = Column(JSON, nullable=True)

    created_at = Column(Date, default=datetime.now)



==================================================
FILE_PATH: app\report\report_service.py
==================================================

import json
import asyncio
import time
import traceback

from datetime import date, datetime, timedelta
from sqlalchemy import func
from app.core.db import SessionLocal, fetch_all
from app.report.report_schema import StoreReport
from app.clients.genai import genai_generate_text
from app.order.order_service import select_daily_sales_by_store
from app.review.review_service import select_reviews_by_store
from app.core.cache import get_report_cache, set_report_cache, get_report_object_cache
from app.report.report_graph import report_graph_app


# ------------------------------------------------------------------
# [Portfolio] Redis vs DB Speed Race Helper Functions (Flattened)
# ------------------------------------------------------------------

async def _measure_redis_speed(s_id: int, check_date: date):
    """Redis ì¡°íšŒ ì†ë„ ì¸¡ì •"""
    start = time.perf_counter()
    data = await get_report_cache(s_id, check_date)
    dur = time.perf_counter() - start
    return dur, data


async def _measure_db_speed(s_id: int, check_date: date):
    """DB ì¡°íšŒ ì†ë„ ì¸¡ì •"""
    start = time.perf_counter()
    row = await select_latest_report(s_id)
    data = None
    if row and str(row['report_date']) == str(check_date):
        data = row
    dur = time.perf_counter() - start
    return dur, data


async def race_condition_check(s_id: int, t_date: str):
    """
    Redisì™€ DBì˜ ì¡°íšŒ ì†ë„ë¥¼ ê²½ìŸ(Race)ì‹œí‚¤ëŠ” ë©”ì¸ ë¡œì§.
    asyncio.gatherë¥¼ ì‚¬ìš©í•˜ì—¬ ë‘ íƒœìŠ¤í¬ë¥¼ ë™ì‹œì— ì‹¤í–‰í•¨.
    """
    logs = []
    
    if t_date:
        check_date = datetime.strptime(t_date, "%Y-%m-%d").date()
    else:
        check_date = date.today()

    # Async Execution (Race Start!) ğŸ”«
    # í—¬í¼ í•¨ìˆ˜ë“¤ì„ ë™ì‹œì— í˜¸ì¶œ
    (redis_time, redis_data), (db_time, db_data) = await asyncio.gather(
        _measure_redis_speed(s_id, check_date), 
        _measure_db_speed(s_id, check_date)
    )
    
    data_found = redis_data if redis_data else db_data
    
    # ë°ì´í„°ê°€ ì–´ë”˜ê°€ì— ìˆë‹¤ë©´ Race Log ìƒì„±
    if data_found:
        winner = "Redis" if redis_time < db_time else "DB"
        gap = db_time / redis_time if redis_time > 0 else 99.9
        
        logs.append(f"ğŸï¸ [Race] {winner} Win! (Redis: {redis_time:.4f}s vs DB: {db_time:.4f}s)")
        logs.append(f"âš¡ [ì†ë„ ë¹„êµ] Redisê°€ DBë³´ë‹¤ {gap:.1f}ë°° ë” ë¹ ë¦…ë‹ˆë‹¤!")
        
        # DB ë°ì´í„°ë§Œ ìˆëŠ” ê²½ìš° í¬ë§· ë§ì¶¤ (UI í˜¸í™˜ì„±)
        if not redis_data and db_data:
                # DB Rowë¥¼ Dict êµ¬ì¡°ë¡œ ê°ì‹¸ê¸°
                data_found = {"report": db_data, "logs": [], "mode": "sequential"}

    return data_found, logs


# ------------------------------------------------------------------
# Main Service Function
# ------------------------------------------------------------------

async def generate_ai_store_report(store_id: int, store_name: str, mode: str = "sequential", target_date: str = None):
    """
    LangGraph í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ (Sequential Graph)
    ìºì‹œ í™•ì¸ â†’ ì—†ìœ¼ë©´ ìƒì„± â†’ ìºì‹œ ì €ì¥
    """
    try:
        print(f"ğŸš€ [Service] '{store_name}' ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘ ({target_date if target_date else 'Today'})...")

        # 1. [Race] ìºì‹œ/DB ê²½ìŸ ì¡°íšŒ (Flattened êµ¬ì¡°)
        cached_data, race_logs = await race_condition_check(store_id, target_date)
        
        if cached_data:
            print(f"â™»ï¸ [Service] '{store_name}' ë¦¬í¬íŠ¸ ì¡°íšŒ ì„±ê³µ! (Race Winner Logic)")
            
            # ê¸°ì¡´ ë¡œê·¸ì— ë ˆì´ìŠ¤ ë¡œê·¸ ë³‘í•©
            final_logs = race_logs + cached_data.get("logs", [])
            cached_data["logs"] = final_logs
            cached_data["cached"] = True
            return cached_data

        # 2. ë¦¬í¬íŠ¸ ìƒì„± (ë°ì´í„° ì—†ìŒ -> AI ì‹¤í–‰)
        initial_state = {
            "store_id": store_id,
            "store_name": store_name,
            "target_date": target_date, # [NEW] ë¶„ì„ ëŒ€ìƒ ë‚ ì§œ
            "execution_logs": race_logs # Race ê²°ê³¼(ì—†ìŒ)ë„ ë¡œê·¸ì— ë‚¨ê¹€
        }

        # LangGraph ì‹¤í–‰ (ë¯¸ë¦¬ ì»´íŒŒì¼ëœ ì‹±ê¸€í†¤ ì•± ì‚¬ìš©)
        final_state = await report_graph_app.ainvoke(initial_state)

        # DBì—ì„œ ì €ì¥ëœ ë¦¬í¬íŠ¸ ì¡°íšŒ
        report = await select_latest_report(store_id)

        # ì‹¤í–‰ ë¡œê·¸ ìˆ˜ì§‘
        logs = race_logs + final_state.get("execution_logs", [])

        result = {
            "report": report,
            "logs": logs,
            "mode": mode,
            "cached": False
        }

        # 3. ìƒì„±ëœ ë¦¬í¬íŠ¸ë¥¼ ìºì‹œì— ì €ì¥ (Redis + DBëŠ” ì´ë¯¸ ìœ„ì—ì„œ ë¨)
        # target_dateê°€ ìˆìœ¼ë©´ ê·¸ê±¸ë¡œ, ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œë¡œ key ìƒì„±
        save_date = datetime.strptime(target_date, "%Y-%m-%d").date() if target_date else date.today()
        
        # [Prevent Caching Bad Data] ë¶ˆëŸ‰ ë¦¬í¬íŠ¸(Risk Score=0)ëŠ” Redis ì €ì¥ ê±´ë„ˆë›°ê¸°
        risk_check = report.get("risk_assessment") if report else None
        risk_score = risk_check.get("risk_score") if risk_check else 0
        
        if risk_score and risk_score > 0:
            await set_report_cache(store_id, result, save_date)
        else:
            print("âš ï¸ [Cache Skip] ë¶ˆëŸ‰ ë¦¬í¬íŠ¸ë¼ Redis ìºì‹±ì„ ìƒëµí•©ë‹ˆë‹¤.")

        return result

    except Exception as e:
        print(f"âŒ [Service] ì—ëŸ¬ ë°œìƒ: {str(e)}")
        traceback.print_exc()
        return None


async def select_latest_report(store_id: int):
    """
    ì§€ì ì˜ ê°€ì¥ ìµœì‹  ë¦¬í¬íŠ¸ ì¡°íšŒ (DB Only)
    """
    sql = "SELECT * FROM store_reports WHERE store_id = %s ORDER BY report_date DESC, report_id DESC LIMIT 1"
    rows = await fetch_all(sql, (store_id,))
    return rows[0] if rows else None



==================================================
FILE_PATH: app\review\review_router.py
==================================================

from fastapi import APIRouter
from app.review.review_service import select_reviews_by_store

router = APIRouter(prefix="/review", tags=["review"])


@router.get("/store/{store_id}")
async def get_reviews_by_store(store_id: int):
    return await select_reviews_by_store(store_id)



==================================================
FILE_PATH: app\review\review_schema.py
==================================================

from pydantic import BaseModel
from datetime import datetime
from sqlalchemy import Column, Integer, String, Text, DateTime, ForeignKey, Index, Numeric, JSON
from pgvector.sqlalchemy import Vector
from app.core.db import base

# ---------- API / JSON ìš© Pydantic ìŠ¤í‚¤ë§ˆ ----------


class ReviewSchema(BaseModel):
    review_id: int
    store_id: int
    menu_id: int
    rating: int
    review_text: str
    created_at: datetime
    delivery_app: str
    embedding: list[float] | None = None  # 1536 ì°¨ì› ì„ë² ë”©




# ---------- Alembic / DB ë§¤í•‘ìš© SQLAlchemy ëª¨ë¸ ----------


class Review(base):
    __tablename__ = "reviews"

    review_id = Column(Integer, primary_key=True, index=True)
    store_id = Column(Integer, ForeignKey("stores.store_id"), nullable=False)
    order_id = Column(Integer, ForeignKey("orders.order_id"),
                      nullable=True)  # ì–´ë–¤ ì£¼ë¬¸ì— ëŒ€í•œ ë¦¬ë·°ì¸ì§€ ì—°ê²°
    menu_id = Column(Integer, ForeignKey("menus.menu_id"), nullable=False)
    rating = Column(Integer, nullable=False)  # 1~5
    review_text = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.now, nullable=False)
    delivery_app = Column(String(50), nullable=True)  # ë°°ë¯¼/ì¿ íŒ¡ì´ì¸  ë“±

    # AI ê²€ìƒ‰(Semantic Search)ì„ ìœ„í•œ ì„ë² ë”©ì€ ì¡°íšŒ ë¹ˆë„ê°€ ë†’ìœ¼ë¯€ë¡œ ë³¸ í…Œì´ë¸”ì— ìœ ì§€
    embedding = Column(Vector(1536), nullable=True)






==================================================
FILE_PATH: app\review\review_service.py
==================================================

from app.core.db import fetch_all


async def select_reviews_by_store(store_id: int):
    sql = """
        SELECT r.*, m.menu_name, o.ordered_at
        FROM reviews r
        JOIN menus m ON r.menu_id = m.menu_id
        LEFT JOIN orders o ON r.order_id = o.order_id
        WHERE r.store_id = %s
        ORDER BY r.created_at DESC
    """
    rows = await fetch_all(sql, (store_id,))
    return rows



==================================================
FILE_PATH: app\sales\sales_schema.py
==================================================

from pydantic import BaseModel
from datetime import date
from sqlalchemy import Column, Integer, Date, Numeric, ForeignKey, UniqueConstraint, String
from app.core.db import base

# ---------- API / JSON ìš© Pydantic ìŠ¤í‚¤ë§ˆ ----------


class SalesDailySchema(BaseModel):
    sales_id: int | None = None
    store_id: int
    sale_date: date
    total_sales: float
    total_orders: int

# ---------- Alembic / DB ë§¤í•‘ìš© SQLAlchemy ëª¨ë¸ ----------


class SalesDaily(base):
    __tablename__ = "sales_daily"

    sales_id = Column(Integer, primary_key=True, index=True)
    store_id = Column(Integer, ForeignKey("stores.store_id"), nullable=False)
    sale_date = Column(Date, nullable=False)
    total_sales = Column(Numeric(15, 2), default=0)
    total_orders = Column(Integer, default=0)
    
    # ë‚ ì”¨ ì •ë³´ ì¶”ê°€ (ë¹„, ë§‘ìŒ ë“±)
    weather_info = Column(String(50), nullable=True)

    # í•œ ë§¤ì¥ì˜ ê°™ì€ ë‚ ì§œ ë°ì´í„°ëŠ” í•˜ë‚˜ë§Œ ì¡´ì¬í•´ì•¼ í•¨ (ì¤‘ë³µ ë°©ì§€)
    __table_args__ = (
        UniqueConstraint('store_id', 'sale_date', name='uix_store_date'),
    )



==================================================
FILE_PATH: app\store\store_router.py
==================================================

from fastapi import APIRouter
from app.store.store_service import select_stores_all

router = APIRouter()

@router.get("/store/get")
async def get_stores_all():
    results = await select_stores_all()
    return results


==================================================
FILE_PATH: app\store\store_schema.py
==================================================

from pydantic import BaseModel
from datetime import date
from sqlalchemy import Column, Integer, String, Float, Date, Numeric
from app.core.db import base

# ---------- API / JSON ìš© Pydantic ìŠ¤í‚¤ë§ˆ ----------


class StoreSchema(BaseModel):
    store_id: int
    store_name: str
    region: str
    city: str
    lat: float
    lon: float
    population_density_index: float | None = None
    open_date: date | None = None
    franchise_type: str | None = None


# ---------- Alembic / DB ë§¤í•‘ìš© SQLAlchemy ëª¨ë¸ ----------

class Store(base):
    __tablename__ = "stores"

    store_id = Column(Integer, primary_key=True, index=True)
    store_name = Column(String(100), nullable=False)
    region = Column(String(50), nullable=False)           # ì„œìš¸ / ëŒ€êµ¬ / ê°•ì›
    city = Column(String(50), nullable=False)             # UIÂ·ì§€ë„ í‘œì‹œìš©
    # ìœ„ë„ (DOUBLE PRECISION)
    lat = Column(Float, nullable=False)
    # ê²½ë„ (DOUBLE PRECISION)
    lon = Column(Float, nullable=False)
    population_density_index = Column(Float, nullable=True)  # ë„ì‹¬ ëŒ€ë¹„ ì¸êµ¬ë°€ë„ ì§€ìˆ˜
    open_date = Column(Date, nullable=True)
    franchise_type = Column(String(20), nullable=True)    # ì§ì˜ / ê°€ë§¹



==================================================
FILE_PATH: app\store\store_service.py
==================================================

from app.core.db import fetch_all

async def select_stores_all():
    sql = "SELECT * FROM stores"
    rows = await fetch_all(sql)
    return rows


==================================================
FILE_PATH: app\user\user_router.py
==================================================

from fastapi import APIRouter, Body

from app.user.user_schema import UserLogin, Users
from app.user.user_service import user_insert, user_select_byemail

router = APIRouter()

@router.post("/user/create")
async def user_create(body : Users):
  response = await user_insert(body)
  return response

@router.post("/user/login")
async def user_get_byemail(body : UserLogin):
  response = await user_select_byemail(body)
  return response


# @router.post("/user/create")
# async def user_create(user_id : str = Body(...) ):
#   response = await user_insert(user_id)
#   return response


==================================================
FILE_PATH: app\user\user_schema.py
==================================================

from pydantic import BaseModel

from sqlalchemy import Column, Integer, String, DateTime, func

from app.core.db import base


# ---------- API / JSON ìš© Pydantic ìŠ¤í‚¤ë§ˆ ----------

class Users(BaseModel):
  email: str
  nickname: str
  password_hash: str


class UserLogin(BaseModel):
  email: str


# ---------- Alembic / DB ë§¤í•‘ìš© SQLAlchemy ëª¨ë¸ ----------

class User(base):
  __tablename__ = "users"

  id = Column(Integer, primary_key=True, index=True)
  email = Column(String, unique=True, nullable=False, index=True)
  password_hash = Column(String, nullable=False)
  nickname = Column(String, nullable=False)
  created_at = Column(DateTime(timezone=True), server_default=func.now())



==================================================
FILE_PATH: app\user\user_service.py
==================================================

from app.core.db import execute_return, fetch_one
from app.core.response import Response
from app.user.user_router import Users
from app.user.user_schema import UserLogin

async def user_insert(body : Users):
  user_id = await execute_return("""
    insert into users (email, nickname)
    values (%s, %s)
    returning id;
  """,(body.email, body.nickname))
  print("user_insert user_id ê°’ì€?", user_id)
  return "user_insert ì™„ë£Œ user_idëŠ”", user_id["id"]

async def user_select_byemail(body : UserLogin):
  row = await fetch_one("""
    select id, email, nickname
    from users
    where email = %s
  """,(body.email,))

  if(row):
    return Response(success=True, data=row["id"], message="user success")
  return Response(success=False, data="",message="user not found")


==================================================
FILE_PATH: app\util\__init__.py
==================================================

"""
ìœ í‹¸ë¦¬í‹° ëª¨ë“ˆ
ê³µí†µìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
"""








==================================================
FILE_PATH: app\util\decorators.py
==================================================

import time
import functools

def perform_async_logging(func):
    """
    [ë¹„ë™ê¸° í•¨ìˆ˜ìš©]
    ì‹¤í–‰ ì‹œê°„ ì¸¡ì • + ì—ëŸ¬ í•¸ë“¤ë§ì„ ìë™ìœ¼ë¡œ í•´ì£¼ëŠ” ë°ì½”ë ˆì´í„°
    """
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        func_name = func.__name__
        print(f"ğŸš€ [Start] {func_name}")
        start_time = time.perf_counter()
        
        try:
            # ì‹¤ì œ í•¨ìˆ˜ ì‹¤í–‰!
            result = await func(*args, **kwargs)
            return result
            
        except Exception as e:
            # ì—ëŸ¬ ë‚˜ë©´ ì—¬ê¸°ì„œ ì¡í˜
            print(f"ğŸ’¥ [Error] {func_name} ì¤‘ë‹¨: {e}")
            return None # í˜¹ì€ raise e
            
        finally:
            # ì„±ê³µí•˜ë“  ì‹¤íŒ¨í•˜ë“  ë¬´ì¡°ê±´ ì‹¤í–‰
            duration = time.perf_counter() - start_time
            print(f"ğŸ [End] {func_name} (â±ï¸ {duration:.3f}s)")
            
    return wrapper

