import json
from typing import Annotated, TypedDict, List, Dict, Any
from datetime import date
from langgraph.graph import StateGraph, END
from app.core.db import SessionLocal
from app.report.report_schema import StoreReport
from app.order.order_service import select_daily_sales_by_store, select_menu_sales_comparison, select_sales_by_day_type
from app.review.review_service import select_reviews_by_store
from app.clients.genai import genai_generate_text
from app.clients.weather import fetch_weather_data

from langgraph.graph.message import add_messages

# ë¦¬ìŠ¤íŠ¸ë¥¼ ë®ì–´ì“°ì§€ ì•Šê³  ì¶”ê°€í•˜ê¸° ìœ„í•œ ë¦¬ë“€ì„œ í•¨ìˆ˜
def append_logs(left: List[str], right: List[str]) -> List[str]:
    return left + right

# 1. ê·¸ë˜í”„ ìƒíƒœ(State) ì •ì˜
class ReportState(TypedDict):
    store_id: int
    store_name: str
    target_date: str # [Optional] ë¶„ì„ ê¸°ì¤€ ë‚ ì§œ (YYYY-MM-DD)
    sales_data: List[Dict[str, Any]]      # ì´ë²ˆì£¼ ë§¤ì¶œ (ìµœê·¼ 7ì¼)
    prev_sales_data: List[Dict[str, Any]] # ì§€ë‚œì£¼ ë§¤ì¶œ (ê·¸ ì „ 7ì¼)
    reviews_data: List[Dict[str, Any]]
    menu_sales_data: List[Dict[str, Any]]
    weather_data: Dict[str, str]
    # [NEW] ì§‘ê³„ ì •í•©ì„±ì„ ìœ„í•´ fetch ë‹¨ê³„ì—ì„œ ê³„ì‚°í•œ ê°’ì„ ë„˜ê¹€
    calculated_total_sales: float 
    calculated_prev_sales: float
    final_report: Dict[str, Any]
    execution_logs: Annotated[List[str], append_logs]

async def fetch_store_data(store_id: int):
    pass

async def fetch_data_node(state: ReportState):
    """DBì—ì„œ ë§¤ì¶œê³¼ ë¦¬ë·° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë…¸ë“œ"""
    store_id = state["store_id"]
    log = f"ğŸ“Š [Fetch] '{state['store_name']}' ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"
    print(log)

    # 1. ê¸°ì¤€ ë‚ ì§œ(Anchor Date) ê²°ì •
    # ì‹œì—° ëª¨ë“œ or ê³¼ê±° ë‚ ì§œ ì¡°íšŒ ì§€ì›
    from app.core.db import fetch_all
    from datetime import datetime, timedelta
    
    target_date_str = state.get("target_date")
    
    if not target_date_str:
        # íƒ€ê²Ÿ ë‚ ì§œê°€ ì—†ìœ¼ë©´ DB ìµœì‹  ë‚ ì§œ ì¡°íšŒ (Simulation Mode)
        max_date_query = f"SELECT MAX(sale_date) as last_date FROM sales_daily WHERE store_id = {store_id}"
        try:
            max_date_rows = await fetch_all(max_date_query)
            if max_date_rows and max_date_rows[0]['last_date']:
                target_date_str = str(max_date_rows[0]['last_date'])
                log += f"\nğŸ•’ ìµœì‹  ë°ì´í„° ë‚ ì§œ ê¸°ì¤€: {target_date_str}"
            else:
                target_date_str = str(date.today())
        except:
            target_date_str = str(date.today())

    ref_date = datetime.strptime(target_date_str, "%Y-%m-%d").date()
    
    # 2. ë°ì´í„° ì¡°íšŒ
    # ë©”ë‰´ë³„, ìš”ì¼ë³„ í†µê³„ëŠ” ê¸°ì¤€ ë‚ ì§œë¥¼ ë„˜ê²¨ì„œ DBì—ì„œ ì •í™•íˆ ê³„ì‚°
    menu_stats = await select_menu_sales_comparison(store_id, days=7, target_date=target_date_str)
    # day_stats = await select_sales_by_day_type(store_id, days=7, target_date=target_date_str) # [ì‚­ì œ] DB í˜¸ì¶œ ëŒ€ì‹  ì§ì ‘ ì§‘ê³„
    reviews = await select_reviews_by_store(store_id) # ë¦¬ë·°ëŠ” ì „ì²´ ê°€ì ¸ì™€ì„œ ìµœì‹ ìˆœ (TODO: ë‚ ì§œ í•„í„°ë§?)

    # ì¼ë³„ ë§¤ì¶œì€ ì „ì²´ë¥¼ ê°€ì ¸ì˜¨ ë’¤ íŒŒì´ì¬ì—ì„œ ë‚ ì§œ í•„í„°ë§ (DB í˜¸ì¶œ íšŸìˆ˜ ì ˆì•½)
    all_sales = await select_daily_sales_by_store(store_id)
    
    # 3. ë‚ ì§œ í•„í„°ë§ (ì´ë²ˆì£¼ vs ì§€ë‚œì£¼)
    # ì´ë²ˆì£¼: ref_date í¬í•¨ ìµœê·¼ 7ì¼ (ref_date - 6 ~ ref_date)
    # ì§€ë‚œì£¼: ê·¸ ì „ 7ì¼ (ref_date - 13 ~ ref_date - 7)
    
    curr_start = ref_date - timedelta(days=6)
    curr_end = ref_date
    prev_start = ref_date - timedelta(days=13)
    prev_end = ref_date - timedelta(days=7)
    
    target_sales = []
    prev_sales = []

    # [NEW] íŒŒì´ì¬ ë‚´ë³´ë‚´ê¸° ì§‘ê³„ (í‰ì¼/ì£¼ë§ ì •í•©ì„± ë³´ì¥)
    weekday_sales = {"recent": 0, "prev": 0}
    weekend_sales = {"recent": 0, "prev": 0}
    
    for s in all_sales:
        s_date = s['order_date'] # date object
        rev = float(s['daily_revenue'])

        # ì´ë²ˆì£¼ ë°ì´í„°
        if curr_start <= s_date <= curr_end:
            target_sales.append(s)
            if s_date.weekday() < 5: # 0~4: í‰ì¼
                weekday_sales["recent"] += rev
            else: # 5~6: ì£¼ë§
                weekend_sales["recent"] += rev

        # ì§€ë‚œì£¼ ë°ì´í„°
        elif prev_start <= s_date <= prev_end:
            prev_sales.append(s)
            if s_date.weekday() < 5:
                weekday_sales["prev"] += rev
            else:
                weekend_sales["prev"] += rev
            
    # ì •ë ¬ (ë‚ ì§œ ì˜¤ë¦„ì°¨ìˆœ) -> ê·¸ë˜í”„ìš©
    target_sales.sort(key=lambda x: x['order_date'])
    prev_sales.sort(key=lambda x: x['order_date'])

    # weather_map êµ¬ì„±
    weather_map = {str(s['order_date']): s.get('weather_info', 'ì•Œìˆ˜ì—†ìŒ') for s in target_sales}

    return {
        "sales_data": target_sales,
        "prev_sales_data": prev_sales,
        "reviews_data": reviews[:15], # ìµœì‹  15ê°œë§Œ ì‚¬ìš©
        "menu_sales_data": menu_stats,
        "weather_data": weather_map,
        "calculated_total_sales": weekday_sales["recent"] + weekend_sales["recent"], # [NEW] ì •í™•í•œ í•©ê³„ ì „ë‹¬
        "calculated_prev_sales": weekday_sales["prev"] + weekend_sales["prev"],
        "target_date": target_date_str, # State ì—…ë°ì´íŠ¸
        "execution_logs": [log, f"âœ… [Fetch] ë°ì´í„° ìˆ˜ì§‘ ë° ì •í•©ì„± ê²€ì¦ ì™„ë£Œ (ê¸°ì¤€ì¼: {target_date_str})"]
    }

async def analyze_data_node(state: ReportState):
    """ë°ì´í„° ë¶„ì„ ë° ìˆ˜ì¹˜ì  ê·¼ê±° ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ë…¸ë“œ"""
    log = "ğŸ§  [Analyze] ìˆ˜ì¹˜ ë°ì´í„° ê³„ì‚° ë° AI ë¶„ì„ ì‹œì‘"
    print(log)

    sales = state["sales_data"]     # ì´ë²ˆì£¼
    prev_sales = state.get("prev_sales_data", []) # ì§€ë‚œì£¼
    reviews = state["reviews_data"]
    menu_stats = state.get("menu_sales_data", [])
    
    # 1. ì£¼ê°„ ë§¤ì¶œ ë¹„êµ (Weekly Comparison)
    # [ë³€ê²½] fetch ë‹¨ê³„ì—ì„œ ê³„ì‚°ëœ ì •í™•í•œ í•©ê³„ ì‚¬ìš© (ì¬ê³„ì‚° X)
    this_week_total = state["calculated_total_sales"]
    prev_week_total = state.get("calculated_prev_sales", 0)
    
    avg_rev = this_week_total / len(sales) if sales else 0
    
    # ì„±ì¥ë¥  ê³„ì‚° (Growth Rate)
    if prev_week_total > 0:
        growth_rate = ((this_week_total - prev_week_total) / prev_week_total * 100)
    else:
        growth_rate = 100 if this_week_total > 0 else 0

    avg_rating = sum(r['rating'] for r in reviews) / len(reviews) if reviews else 0

    # ë©”ë‰´ë³„ ì¦ê° ë¶„ì„ (ë§¤ì¶œ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ëœ ìƒíƒœ)
    processed_menus = []
    for m in menu_stats:
        rec_rev = float(m['recent_revenue'])
        prev_rev = float(m['prev_revenue'])
        change_pct = ((rec_rev - prev_rev) / prev_rev * 100) if prev_rev > 0 else (100 if rec_rev > 0 else 0)
        
        item = {
            "menu": m['menu_name'],
            "cat": m['category'],
            "recent_rev": rec_rev,
            "prev_rev": prev_rev,
            "change_pct": round(change_pct, 1)
        }
        processed_menus.append(item)
    
    # 1. Top Selling (ë§¤ì¶œì•¡ ìƒìœ„)
    top_selling = sorted(processed_menus, key=lambda x: x['recent_rev'], reverse=True)[:5]

    # 2. Top Dropping (ê°ì†Œí­ í•˜ìœ„) - ì—­ì„±ì¥ ë©”ë‰´
    dropping_candidates = [m for m in processed_menus if m['prev_rev'] > 0]
    worst_dropping = sorted(dropping_candidates, key=lambda x: x['change_pct'])[:5]

    # UIìš© ì›ë³¸ ë°ì´í„° ìš”ì•½ (ë‚ ì§œ, ë§¤ì¶œë§Œ) + ë‚ ì”¨ ì¶”ê°€
    source_sales = []
    for s in sales:
        d_str = str(s['order_date'])
        source_sales.append({
            "date": d_str,
            "revenue": float(s['daily_revenue']),
            "weather": s.get('weather_info', "ì•Œìˆ˜ì—†ìŒ")
        })

    prompt = f"""
    í”„ëœì°¨ì´ì¦ˆ ê²½ì˜ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  **ìˆ˜ì¹˜ì  ê·¼ê±°**ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ê²°ì±…ì„ ì œì‹œí•´ì¤˜.
    ë§¤ì¥: {state['store_name']}
    
    [ì£¼ê°„ ë§¤ì¶œ ìš”ì•½]
    - ì´ë²ˆì£¼ ì´ ë§¤ì¶œ(7ì¼): {int(this_week_total):,}ì›
    - ì§€ë‚œì£¼ ì´ ë§¤ì¶œ(7ì¼): {int(prev_week_total):,}ì›
    - ì£¼ê°„ ì„±ì¥ë¥ (WoW): {growth_rate:+.1f}%
    - ì´ë²ˆì£¼ í‰ê·  ë³„ì : {avg_rating:.1f}ì 
    
    ìƒì„¸ ë§¤ì¶œ ë‚´ì—­ (ë‚ ì”¨ í¬í•¨): {json.dumps(source_sales, ensure_ascii=False)}
    ìƒì„¸ ë¦¬ë·° ë‚´ì—­: {json.dumps([{"rate": r['rating'], "txt": r['review_text']} for r in reviews], ensure_ascii=False)}
    
    [ë©”ë‰´ ë¶„ì„]
    ì˜ íŒ”ë¦° ë©”ë‰´ (TOP 5): {json.dumps(top_selling, ensure_ascii=False)}
    ê¸‰ê°í•œ ë©”ë‰´ (WORST 5): {json.dumps(worst_dropping, ensure_ascii=False)}

    ë¶„ì„ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ë°˜ë“œì‹œ ì§€ì¼œì¤˜:
    1. **"ì´ë²ˆì£¼ ë§¤ì¶œì´ ì§€ë‚œì£¼ ëŒ€ë¹„ ì™œ ë³€í–ˆëŠ”ê°€?"**ë¥¼ í•µì‹¬ ì£¼ì œë¡œ ì¡ìœ¼ì„¸ìš”. (ì„±ì¥ ë˜ëŠ” í•˜ë½ì˜ ì›ì¸ ê·œëª…)
    2. **ë‚ ì”¨ì™€ ë§¤ì¶œì˜ ìƒê´€ê´€ê³„**ë¥¼ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ì„¸ìš”. 
       - "ì§€ë‚œì£¼ ëŒ€ë¹„ ë¹„ì˜¤ëŠ” ë‚ ì´ ë§ì•„ ë°°ë‹¬ ë§¤ì¶œì´ ëŠ˜ì—ˆë‹¤" ë“± êµ¬ì²´ì ìœ¼ë¡œ.
    3. ìˆ˜ì¹˜ì  ê·¼ê±°(Top 5 ë©”ë‰´ëª…, ì£¼ë§ ë§¤ì¶œ ë³€ë™ë¥  ë“±)ë¥¼ í¬í•¨í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ì‹œê°í™”í•˜ì„¸ìš”.
    4. ëª¨ë“  ì¤„ë°”ê¿ˆ(ê°œí–‰)ì€ ì‹¤ì œ ì¤„ë°”ê¿ˆ ëŒ€ì‹  '\\n' ë¬¸ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. (JSON í¬ë§· ì¤€ìˆ˜)
    
    ì‘ë‹µì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ í•  ê²ƒ:
    {{
        "data_evidence": {{
            "sales_analysis": "ì£¼ê°„ ë§¤ì¶œ ë¹„êµ, ë‚ ì”¨, ë©”ë‰´ ë°ì´í„°ë¥¼ ì¢…í•©í•œ ìƒì„¸ ë¶„ì„ (ë§ˆí¬ë‹¤ìš´ í‘œ í¬í•¨ í•„ìˆ˜)"
        }},
        "summary": "í•µì‹¬ ìš”ì•½ (ì§€ë‚œì£¼ ëŒ€ë¹„ ë³€ë™ ì›ì¸ í¬í•¨ 3ì¤„)",
        "marketing_strategy": "ë‹¤ìŒì£¼ ë§¤ì¶œ ì¦ëŒ€ë¥¼ ìœ„í•œ ë‚ ì”¨/íŠ¸ë Œë“œ ê¸°ë°˜ ë§ˆì¼€íŒ… ì œì•ˆ",
        "operational_improvement": "ë§¤ì¥ ìš´ì˜ íš¨ìœ¨í™” ë° ì„œë¹„ìŠ¤ ê°œì„  ì œì•ˆ",
        "risk_assessment": {{"risk_score": 80, "main_risks": [], "suggestion": ""}}
    }}
    """

    raw_text = await genai_generate_text(prompt)
    
    # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œë¸”ë¡ ì œê±°
    if "```json" in raw_text:
        raw_text = raw_text.split("```json")[1].split("```")[0].strip()
    elif "```" in raw_text:
        raw_text = raw_text.split("```")[1].split("```")[0].strip()
        
    # 2. ì œì–´ ë¬¸ì(Control Characters) ì œê±° (JSON íŒŒì‹± ì—ëŸ¬ ë°©ì§€)
    import re
    # \n(\x0A), \t(\x09), \r(\x0D)ì€ ì‚´ë¦¬ê³  ê·¸ ì™¸ì˜ ì œì–´ ë¬¸ìë§Œ ì œê±° (ë§ˆí¬ë‹¤ìš´ í‘œ ë³´ì¡´)
    cleaned_json = re.sub(r'[\x00-\x08\x0B-\x0C\x0E-\x1F\x7F]', '', raw_text)
    
    # [NEW] Trailing Comma ì œê±° (Standard JSON í˜¸í™˜ì„± í™•ë³´)
    # ì˜ˆ: {"a": 1,} -> {"a": 1}
    cleaned_json = re.sub(r',\s*([}\]])', r'\1', cleaned_json)
    
    try:
        # dirtyjson ëŒ€ì‹  í‘œì¤€ json ì‚¬ìš© + ì •ê·œì‹ ì „ì²˜ë¦¬
        report_dict = json.loads(cleaned_json, strict=False)
    except Exception:
        try:
            # 2ì°¨ ì‹œë„: ì—­ìŠ¬ë˜ì‹œ ì´ì¤‘ ì¹˜í™˜ í›„ ì¬ì‹œë„
            cleaned_json_fixed = cleaned_json.replace('\\', '\\\\')
            report_dict = json.loads(cleaned_json_fixed, strict=False)
        except Exception as e:
            print(f"âŒ [Analyze] JSON íŒŒì‹± ìµœì¢… ì‹¤íŒ¨: {e}")
            print("--- [AI Raw Output Start] ---")
            print(raw_text) # ì „ì²´ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            print("--- [AI Raw Output End] ---")
            
            # Fallback: ê¸°ë³¸ ë¹ˆ í…œí”Œë¦¿ ë°˜í™˜
            report_dict = {
                "data_evidence": {"sales_analysis": "ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨ (AI ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜)"},
                "summary": "ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "marketing_strategy": "",
                "operational_improvement": "",
                "risk_assessment": {"risk_score": 0, "main_risks": [], "suggestion": ""}
            }
    
    # UIìš© í†µê³„ ë°ì´í„° ë° ì†ŒìŠ¤ ë°ì´í„° ì¶”ê°€
    report_dict["metrics"] = {
        "total_rev": this_week_total,
        "avg_rev": avg_rev,
        "trend_percent": growth_rate, # íŠ¸ë Œë“œ ëŒ€ì‹  ì„±ì¥ë¥  ì‚¬ìš©
        "avg_rating": avg_rating,
        "prev_total_rev": prev_week_total # ì§€ë‚œì£¼ ë§¤ì¶œ ì¶”ê°€
    }
    report_dict["source_data"] = {
        "recent_sales": source_sales,
        "review_count": len(reviews),
        "top_selling_menus": top_selling,
        "worst_dropping_menus": worst_dropping,
    }

    return {
        "final_report": report_dict,
        "execution_logs": [log, f"âœ… [Analyze] ìˆ˜ì¹˜ ê·¼ê±° ë¶„ì„ ì™„ë£Œ (ì£¼ê°„ ì„±ì¥ë¥ : {growth_rate:+.1f}%)"]
    }

async def save_report_node(state: ReportState):
    """ìµœì¢… ë¦¬í¬íŠ¸ë¥¼ DBì— ì €ì¥í•˜ëŠ” ë…¸ë“œ"""
    log = "ğŸ’¾ [Save] ë¶„ì„ ê²°ê³¼ DB ì €ì¥ ì¤‘"
    report_dict = state["final_report"]

    # ë©”íŠ¸ë¦­ ë° ì†ŒìŠ¤ ì •ë³´ë¥¼ risk_assessment ë‚´ë¶€ì— ë³‘í•©í•˜ì—¬ ì˜êµ¬ ì €ì¥
    risk_info = report_dict.get('risk_assessment', {})
    risk_info['metrics'] = report_dict.get('metrics')
    risk_info['data_evidence'] = report_dict.get('data_evidence')
    risk_info['source_data'] = report_dict.get('source_data')  # ì›ë³¸ ë°ì´í„° ì¶”ê°€ ì €ì¥

    db_report = StoreReport(
        store_id=state["store_id"],
        report_date=date.today(),
        report_type="AI_GRAPH_REPORT",
        summary=report_dict['summary'],
        marketing_strategy=report_dict['marketing_strategy'],
        operational_improvement=report_dict['operational_improvement'],
        risk_assessment=risk_info
    )

    with SessionLocal() as session:
        session.query(StoreReport).filter_by(
            store_id=state["store_id"],              
            report_date=date.today()).delete()
        session.add(db_report)
        session.commit()

    return {
        "execution_logs": [log, "ğŸ [Complete] í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ"]
    }

def create_report_graph():
    workflow = StateGraph(ReportState)
    workflow.add_node("fetch_data", fetch_data_node)
    workflow.add_node("analyze_data", analyze_data_node)
    workflow.add_node("save_report", save_report_node)

    workflow.set_entry_point("fetch_data")
    workflow.add_edge("fetch_data", "analyze_data")
    workflow.add_edge("analyze_data", "save_report")
    workflow.add_edge("save_report", END)

    return workflow.compile()

# [Singleton íŒ¨í„´] ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì»´íŒŒì¼í•˜ì—¬ ì¬ì‚¬ìš©
report_graph_app = create_report_graph()
