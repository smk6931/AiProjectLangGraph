import json
from typing import Annotated, TypedDict, List, Dict, Any
from datetime import date
from langgraph.graph import StateGraph, END
from app.core.db import SessionLocal
from app.report.report_schema import StoreReport
from app.order.order_service import select_daily_sales_by_store, select_menu_sales_comparison, select_sales_by_day_type
from app.review.review_service import select_reviews_by_store
from app.clients.genai import genai_generate_text
from app.clients.weather import fetch_weather_data

from langgraph.graph.message import add_messages

# ë¦¬ìŠ¤íŠ¸ë¥¼ ë®ì–´ì“°ì§€ ì•Šê³  ì¶”ê°€í•˜ê¸° ìœ„í•œ ë¦¬ë“€ì„œ í•¨ìˆ˜
def append_logs(left: List[str], right: List[str]) -> List[str]:
    return left + right

# 1. ê·¸ë˜í”„ ìƒíƒœ(State) ì •ì˜
class ReportState(TypedDict):
    store_id: int
    store_name: str
    sales_data: List[Dict[str, Any]]
    reviews_data: List[Dict[str, Any]]
    menu_sales_data: List[Dict[str, Any]]
    day_sales_data: List[Dict[str, Any]]
    weather_data: Dict[str, str]  # ë‚ ì”¨ ë°ì´í„° { "2024-01-01": "ë§‘ìŒ" }
    final_report: Dict[str, Any]
    execution_logs: Annotated[List[str], append_logs]


async def fetch_store_data(store_id: int):
    # This is report_autonomous.py tool, but I am editing report_graph.py nodes.
    # report_graph.py doesn't use @tool. It uses fetch_data_node.
    pass

async def fetch_data_node(state: ReportState):
    """DBì—ì„œ ë§¤ì¶œê³¼ ë¦¬ë·° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë…¸ë“œ"""
    store_id = state["store_id"]
    log = f"ğŸ“Š [Fetch] '{state['store_name']}' ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘"
    print(log)

    sales = await select_daily_sales_by_store(store_id)
    reviews = await select_reviews_by_store(store_id)
    menu_stats = await select_menu_sales_comparison(store_id, days=7)
    day_stats = await select_sales_by_day_type(store_id, days=7)

    # ë‚ ì”¨ ë°ì´í„°ëŠ” salesì— ì´ë¯¸ í¬í•¨ë˜ì–´ ìˆìŒ.
    # salesëŠ” ASC ì •ë ¬ (ê³¼ê±° -> í˜„ì¬)
    # ìµœê·¼ 7ì¼ì¹˜ë§Œ ì˜ë¼ì„œ ì‚¬ìš©
    target_sales = sales[-7:] if len(sales) >= 7 else sales
    
    # weather_map êµ¬ì„± (ê¸°ì¡´ ì½”ë“œ í˜¸í™˜ì„± ìœ ì§€)
    weather_map = {str(s['order_date']): s.get('weather_info', 'ì•Œìˆ˜ì—†ìŒ') for s in target_sales}

    return {
        "sales_data": target_sales,  # ìµœê·¼ 7ì¼
        "reviews_data": reviews[:15],  # ìµœê·¼ ë¦¬ë·° 15ê°œ
        "menu_sales_data": menu_stats, # ë©”ë‰´ë³„ íŒë§¤ ë°ì´í„°
        "day_sales_data": day_stats,   # ìš”ì¼ë³„(í‰ì¼/ì£¼ë§) ë¶„ì„ ë°ì´í„°
        "weather_data": weather_map,   # ë‚ ì”¨ ë°ì´í„° (ë¶„ì„ ë…¸ë“œì—ì„œ ì‚¬ìš©)
        "execution_logs": [log, f"âœ… [Fetch] ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ (ë‚ ì”¨ ì •ë³´ í¬í•¨)"]
    }

async def analyze_data_node(state: ReportState):
    """ë°ì´í„° ë¶„ì„ ë° ìˆ˜ì¹˜ì  ê·¼ê±° ê³„ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ë…¸ë“œ"""
    log = "ğŸ§  [Analyze] ìˆ˜ì¹˜ ë°ì´í„° ê³„ì‚° ë° AI ë¶„ì„ ì‹œì‘"
    print(log)

    sales = state["sales_data"]
    reviews = state["reviews_data"]
    menu_stats = state.get("menu_sales_data", [])
    day_stats = state.get("day_sales_data", [])
    weather_map = state.get("weather_data", {})

    # ìˆ˜ì¹˜ ë°ì´í„° ì§ì ‘ ê³„ì‚° (salesëŠ” ì´ë¯¸ ìµœê·¼ 7ì¼ì¹˜ë§Œ ë“¤ì–´ì˜´)
    total_rev = sum(float(s['daily_revenue']) for s in sales)
    avg_rev = total_rev / len(sales) if sales else 0
    
    # Trend Calculation (Recent 3 vs Prev 4)
    # sales is ASC [old ... new]
    if len(sales) >= 7:
        rec_val = sum(float(s['daily_revenue']) for s in sales[4:]) / 3  # Last 3
        prev_val = sum(float(s['daily_revenue']) for s in sales[:4]) / 4 # First 4
        trend = ((rec_val - prev_val) / prev_val * 100) if prev_val > 0 else 0
    else:
        trend = 0
        
    avg_rating = sum(r['rating'] for r in reviews) / len(reviews) if reviews else 0

    # ë©”ë‰´ë³„ ì¦ê° ë¶„ì„ (ë§¤ì¶œ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ëœ ìƒíƒœ)
    processed_menus = []
    for m in menu_stats:
        rec_rev = float(m['recent_revenue'])
        prev_rev = float(m['prev_revenue'])
        change_pct = ((rec_rev - prev_rev) / prev_rev * 100) if prev_rev > 0 else (100 if rec_rev > 0 else 0)
        
        item = {
            "menu": m['menu_name'],
            "cat": m['category'],
            "recent_rev": rec_rev,
            "prev_rev": prev_rev,
            "change_pct": round(change_pct, 1)
        }
        processed_menus.append(item)
    
    # 1. Top Selling (ë§¤ì¶œì•¡ ìƒìœ„)
    top_selling = sorted(processed_menus, key=lambda x: x['recent_rev'], reverse=True)[:5]

    # 2. Top Dropping (ê°ì†Œí­ í•˜ìœ„) - ì—­ì„±ì¥ ë©”ë‰´
    dropping_candidates = [m for m in processed_menus if m['prev_rev'] > 0]
    worst_dropping = sorted(dropping_candidates, key=lambda x: x['change_pct'])[:5]

    # ìš”ì¼ë³„(í‰ì¼/ì£¼ë§) ë¶„ì„
    day_analysis = []
    for d in day_stats:
        r_rev = float(d['recent_revenue'])
        p_rev = float(d['prev_revenue'])
        d_trend = ((r_rev - p_rev) / p_rev * 100) if p_rev > 0 else 0
        day_analysis.append({
            "type": d['day_type'],
            "recent": r_rev,
            "prev": p_rev,
            "trend": round(d_trend, 1)
        })

    # UIìš© ì›ë³¸ ë°ì´í„° ìš”ì•½ (ë‚ ì§œ, ë§¤ì¶œë§Œ) + ë‚ ì”¨ ì¶”ê°€
    source_sales = []
    for s in sales:
        d_str = str(s['order_date'])
        source_sales.append({
            "date": d_str,
            "revenue": float(s['daily_revenue']),
            "weather": s.get('weather_info', "ì•Œìˆ˜ì—†ìŒ")
        })

    prompt = f"""
    í”„ëœì°¨ì´ì¦ˆ ê²½ì˜ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  **ìˆ˜ì¹˜ì  ê·¼ê±°**ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•´ê²°ì±…ì„ ì œì‹œí•´ì¤˜.
    ë§¤ì¥: {state['store_name']}
    ì´ ë§¤ì¶œ(7ì¼): {total_rev:,.0f}ì› | ì¼í‰ê· : {avg_rev:,.0f}ì› | ì¶”ì„¸: {trend:+.1f}% | í‰ì : {avg_rating:.1f}
    
    ìƒì„¸ ë§¤ì¶œ ë‚´ì—­ (ë‚ ì”¨ í¬í•¨): {json.dumps(source_sales, ensure_ascii=False)}
    ìƒì„¸ ë¦¬ë·° ë‚´ì—­: {json.dumps([{"rate": r['rating'], "txt": r['review_text']} for r in reviews], ensure_ascii=False)}
    
    [ë©”ë‰´ ë¶„ì„]
    ì˜ íŒ”ë¦° ë©”ë‰´ (TOP 5): {json.dumps(top_selling, ensure_ascii=False)}
    ê¸‰ê°í•œ ë©”ë‰´ (WORST 5): {json.dumps(worst_dropping, ensure_ascii=False)}

    [ìš”ì¼/ì‹œê°„ ë¶„ì„]
    í‰ì¼ vs ì£¼ë§ ë§¤ì¶œ ë³€ë™: {json.dumps(day_analysis, ensure_ascii=False)}

    ë¶„ì„ ì‹œ ë‹¤ìŒ ì‚¬í•­ì„ ë°˜ë“œì‹œ ì§€ì¼œì¤˜:
    1. **"ë¬´ì—‡ì´, ì–¸ì œ, ì™¸ë¶€ì— ì˜í•´ ì•ˆ íŒ”ë ¸ë‚˜?"** ë‹¤ê°ë„ë¡œ ë¶„ì„í•˜ì„¸ìš”.
    2. **ë‚ ì”¨ì™€ ë§¤ì¶œì˜ ìƒê´€ê´€ê³„**ë¥¼ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ì„¸ìš”. 
       - "ë¹„ê°€ ì™”ìŒì—ë„ ë§¤ì¶œì´ ì„ ë°©í–ˆë‹¤" (ê¸ì •) ë˜ëŠ” "ë‚ ì”¨ê°€ ë§‘ì•˜ëŠ”ë°ë„ ë§¤ì¶œì´ ê¸‰ê°í–ˆë‹¤" (ë¶€ì •) ë“±.
    3. ìˆ˜ì¹˜ì  ê·¼ê±°(Top 5 ë©”ë‰´ëª…, ì£¼ë§ ë§¤ì¶œ ë³€ë™ë¥  ë“±)ë¥¼ í¬í•¨í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ í‘œë¡œ ì‹œê°í™”í•˜ì„¸ìš”.
    
    ì‘ë‹µì€ ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ í•  ê²ƒ:
    {{
        "data_evidence": {{
            "sales_analysis": "ë‚ ì”¨, ë©”ë‰´, ìš”ì¼ë³„ ë°ì´í„°ë¥¼ ì¢…í•©í•œ ìƒì„¸ ë§¤ì¶œ ë¶„ì„ (ë§ˆí¬ë‹¤ìš´ í‘œ í¬í•¨ í•„ìˆ˜)",
            "review_analysis": "í‰ì ê³¼ ë¦¬ë·° ê²°í•© ë¶„ì„"
        }},
        "summary": "ì¢…í•© ë¶„ì„ ìš”ì•½ (3ì¤„)",
        "marketing_strategy": "ì™¸ë¶€ ìš”ì¸(ë‚ ì”¨ ë“±)ì„ ê³ ë ¤í•œ ë§ˆì¼€íŒ… ì œì•ˆ",
        "operational_improvement": "ìš´ì˜ ê°œì„  ì œì•ˆ",
        "risk_assessment": {{"risk_score": 80, "main_risks": [], "suggestion": ""}}
    }}
    """

    report_json = await genai_generate_text(prompt)
    if "```json" in report_json:
        report_json = report_json.split("```json")[1].split("```")[0].strip()
    elif "```" in report_json:
        report_json = report_json.split("```")[1].split("```")[0].strip()

    report_dict = json.loads(report_json)
    
    # UIìš© í†µê³„ ë°ì´í„° ë° ì†ŒìŠ¤ ë°ì´í„° ì¶”ê°€
    report_dict["metrics"] = {
        "total_rev": total_rev,
        "avg_rev": avg_rev,
        "trend_percent": trend,
        "avg_rating": avg_rating
    }
    report_dict["source_data"] = {
        "recent_sales": source_sales,
        "review_count": len(reviews),
        "top_selling_menus": top_selling,
        "worst_dropping_menus": worst_dropping,
        "day_analysis": day_analysis
    }

    return {
        "final_report": report_dict,
        "execution_logs": [log, f"âœ… [Analyze] ìˆ˜ì¹˜ ê·¼ê±° ë¶„ì„ ì™„ë£Œ (ì¶”ì„¸: {trend:+.1f}%)"]
    }

async def save_report_node(state: ReportState):
    """ìµœì¢… ë¦¬í¬íŠ¸ë¥¼ DBì— ì €ì¥í•˜ëŠ” ë…¸ë“œ"""
    log = "ğŸ’¾ [Save] ë¶„ì„ ê²°ê³¼ DB ì €ì¥ ì¤‘"
    report_dict = state["final_report"]

    # ë©”íŠ¸ë¦­ ë° ì†ŒìŠ¤ ì •ë³´ë¥¼ risk_assessment ë‚´ë¶€ì— ë³‘í•©í•˜ì—¬ ì˜êµ¬ ì €ì¥
    risk_info = report_dict.get('risk_assessment', {})
    risk_info['metrics'] = report_dict.get('metrics')
    risk_info['data_evidence'] = report_dict.get('data_evidence')
    risk_info['source_data'] = report_dict.get('source_data')  # ì›ë³¸ ë°ì´í„° ì¶”ê°€ ì €ì¥

    db_report = StoreReport(
        store_id=state["store_id"],
        report_date=date.today(),
        report_type="AI_GRAPH_REPORT",
        summary=report_dict['summary'],
        marketing_strategy=report_dict['marketing_strategy'],
        operational_improvement=report_dict['operational_improvement'],
        risk_assessment=risk_info
    )

    with SessionLocal() as session:
        session.query(StoreReport).filter_by(
            store_id=state["store_id"],              
            report_date=date.today()).delete()
        session.add(db_report)
        session.commit()

    return {
        "execution_logs": [log, "ğŸ [Complete] í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ"]
    }

def create_report_graph():
    workflow = StateGraph(ReportState)
    workflow.add_node("fetch_data", fetch_data_node)
    workflow.add_node("analyze_data", analyze_data_node)
    workflow.add_node("save_report", save_report_node)

    workflow.set_entry_point("fetch_data")
    workflow.add_edge("fetch_data", "analyze_data")
    workflow.add_edge("analyze_data", "save_report")
    workflow.add_edge("save_report", END)

    return workflow.compile()
